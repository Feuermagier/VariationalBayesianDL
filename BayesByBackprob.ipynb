{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See https://github.com/nitarshan/bayes-by-backprop/blob/master/Weight%20Uncertainty%20in%20Neural%20Networks.ipynb\n",
    "# The network structure is basically copied, while the learning code has been written mostly by me\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision.transforms import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "\n",
    "transform = transforms.Compose([\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize(0,5, 0.5)\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "# dataiter = iter(trainloader)\n",
    "# images, labels = dataiter.next()\n",
    "# imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multidimensional gaussian distribution; for sampling weights and biases of a nn layer\n",
    "\n",
    "class GaussianParameters:\n",
    "    def __init__(self, mu, rho):\n",
    "        self.mu, self.rho = mu, rho\n",
    "        self.distribution = torch.distributions.Normal(0, 1)\n",
    "\n",
    "    @property\n",
    "    def sigma(self):\n",
    "        return torch.log1p(torch.exp(self.rho))\n",
    "\n",
    "    def sample(self):\n",
    "        epsilon = self.distribution.sample(self.rho.size())\n",
    "        return self.mu + self.sigma * epsilon\n",
    "\n",
    "    def log_prob(self, value):\n",
    "        return (-np.log(np.sqrt(2 * np.pi)) - torch.log(self.sigma) - ((value - self.mu) ** 2) / (2 * self.sigma ** 2)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted sum of two gaussian distributions\n",
    "\n",
    "class GaussianMixture:\n",
    "    def __init__(self, pi, sigma1, sigma2):\n",
    "        self.pi, self.sigma1, self.sigma2 = pi, sigma1, sigma2\n",
    "        self.gaussian1 = torch.distributions.Normal(0, sigma1)\n",
    "        self.gaussian2 = torch.distributions.Normal(0, sigma2)\n",
    "\n",
    "    def log_prob(self, value):\n",
    "        p1 = torch.exp(self.gaussian1.log_prob(value))\n",
    "        p2 = torch.exp(self.gaussian2.log_prob(value))\n",
    "        return torch.log((self.pi * p1 + (1 - self.pi) * p2)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single layer of a BBB network. In essence this is a normal fc layer, but the weights and biases\n",
    "# are drawn from a gaussian distribution and the corresponding means and variances are learned\n",
    "\n",
    "# This class also stores the prior and variational posterior of the weights and biases of the last\n",
    "# forward pass through the layer\n",
    "\n",
    "class BayesianLayer(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, weight_prior: GaussianMixture, bias_prior: GaussianMixture):\n",
    "        super().__init__()\n",
    "        self.in_features, self.out_features = in_features, out_features\n",
    "        self.weight_prior, self.bias_prior = weight_prior, bias_prior\n",
    "        self.log_prior, self.log_posterior = 0, 0\n",
    "\n",
    "        # Weights\n",
    "        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features).uniform_(-0.2, 0.2))\n",
    "        self.weight_rho = nn.Parameter(torch.Tensor(out_features, in_features).uniform_(-5, -4))\n",
    "        self.weight = GaussianParameters(self.weight_mu, self.weight_rho)\n",
    "\n",
    "        # Biases\n",
    "        self.bias_mu = nn.Parameter(torch.Tensor(out_features).uniform_(-0.2, 0.2))\n",
    "        self.bias_rho = nn.Parameter(torch.Tensor(out_features).uniform_(-5, -4))\n",
    "        self.bias = GaussianParameters(self.bias_mu, self.bias_rho)\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.training:\n",
    "            weight = self.weight.sample()\n",
    "            bias = self.bias.sample()\n",
    "            # Log probability of the drawn parameters given the prior\n",
    "            self.log_prior = self.weight_prior.log_prob(weight) + self.bias_prior.log_prob(bias)\n",
    "            # Log probability of the drawn parameters given the parameter distribution\n",
    "            self.log_posterior = self.weight.log_prob(weight) + self.bias.log_prob(bias)\n",
    "        else:\n",
    "            # If we are not in training mode use deterministic parameters (i.e. the means)\n",
    "            weight = self.weight.mu\n",
    "            bias = self.bias.mu\n",
    "        return F.relu(F.linear(input, weight, bias))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BBB network with three layers (and currently fixed input/hidden/output dimensions). Stores also\n",
    "# the prior and variational posterior of the last forward pass\n",
    "\n",
    "class BayesianNetwork(nn.Module):\n",
    "    def __init__(self, weight_prior: GaussianMixture, bias_prior: GaussianMixture):\n",
    "        super().__init__()\n",
    "        self.first = BayesianLayer(28*28, 400, weight_prior, bias_prior)\n",
    "        self.second = BayesianLayer(400, 400, weight_prior, bias_prior)\n",
    "        self.third = BayesianLayer(400, 10, weight_prior, bias_prior)\n",
    "    \n",
    "    @property\n",
    "    def log_prior(self):\n",
    "        return self.first.log_prior + self.second.log_prior + self.third.log_prior\n",
    "\n",
    "    @property\n",
    "    def log_posterior(self):\n",
    "        return self.first.log_posterior + self.second.log_posterior + self.third.log_posterior\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        input = self.first(input)\n",
    "        input = self.second(input)\n",
    "        input = self.third(input)\n",
    "        return F.log_softmax(input, dim=1) # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net: BayesianNetwork, optimizer: torch.optim.Optimizer, loader, batch_count, monte_carlo_samples=1):\n",
    "    net.train(True)\n",
    "    for i, (data, target) in enumerate(loader):\n",
    "        total_loss = 0\n",
    "        batch_size = data.shape[0]\n",
    "\n",
    "        data = torch.flatten(data, start_dim=1)\n",
    "        net.zero_grad()\n",
    "        for sample, target in zip(data, target):\n",
    "            input = torch.unsqueeze(sample, 0)\n",
    "            target = torch.unsqueeze(target, 0)\n",
    "            output = net(input)\n",
    "            log_prior = net.log_prior\n",
    "            log_posterior = net.log_posterior\n",
    "            loss = (log_posterior - log_prior) / batch_count + F.nll_loss(output, target, reduction=\"sum\")\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Completed minibatch #{i}; loss is {total_loss / batch_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = 0.5 # 0.25, 0.5, 0.75\n",
    "sigma1 = np.exp(-1) # 0, 1, 2\n",
    "sigma2 = np.exp(-7) # 6, 7, 8\n",
    "prior = GaussianMixture(pi, sigma1, sigma2)\n",
    "\n",
    "net = BayesianNetwork(prior, prior)\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "train(net, optimizer, trainloader, len(trainloader))\n",
    "torch.save(net.state_dict(), \"models/bbb_mnist_sgd.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9229000210762024\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    net.train(False)\n",
    "    corrects = 0\n",
    "    samples = 0\n",
    "    for data, target in testloader:\n",
    "        data = torch.flatten(data, start_dim=1)\n",
    "        outputs = net(data)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        corrects += ((preds - target) == 0).sum()\n",
    "        samples += len(data)\n",
    "    print(f\"Test accuracy: {corrects / samples}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96836cc28e460061bca45187f20cd83662f914d0b92933e0c7f2f0f17d1b293c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
