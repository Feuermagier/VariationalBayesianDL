{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See https://github.com/nitarshan/bayes-by-backprop/blob/master/Weight%20Uncertainty%20in%20Neural%20Networks.ipynb\n",
    "# The network structure is basically copied, while the learning code has been written mostly by me\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision.transforms import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No cuda device available; using the CPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"No cuda device available; using the CPU\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "\n",
    "transform = transforms.Compose([\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize(0,5, 0.5)\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "# dataiter = iter(trainloader)\n",
    "# images, labels = dataiter.next()\n",
    "# imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multidimensional gaussian distribution; for sampling weights and biases of a nn layer\n",
    "\n",
    "class GaussianParameters:\n",
    "    def __init__(self, mu: torch.Tensor, rho: torch.Tensor):\n",
    "        self.mu, self.rho = mu, rho\n",
    "        self.distribution = torch.distributions.Normal(0, 1)\n",
    "\n",
    "    @property\n",
    "    def sigma(self):\n",
    "        return torch.log1p(torch.exp(self.rho))\n",
    "\n",
    "    def sample(self):\n",
    "        epsilon = self.distribution.sample(self.rho.size()).to(device)\n",
    "        return self.mu + self.sigma * epsilon\n",
    "\n",
    "    def log_prob(self, value):\n",
    "        return (-np.log(np.sqrt(2 * np.pi)) - torch.log(self.sigma) - ((value - self.mu) ** 2) / (2 * self.sigma ** 2)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted sum of two gaussian distributions\n",
    "class GaussianMixture:\n",
    "    def __init__(self, pi, sigma1, sigma2):\n",
    "        self.pi, self.sigma1, self.sigma2 = pi, sigma1, sigma2\n",
    "        self.gaussian1 = torch.distributions.Normal(0, sigma1)\n",
    "        self.gaussian2 = torch.distributions.Normal(0, sigma2)\n",
    "\n",
    "    def log_prob(self, value):\n",
    "        p1 = torch.exp(self.gaussian1.log_prob(value))\n",
    "        p2 = torch.exp(self.gaussian2.log_prob(value))\n",
    "        return torch.log((self.pi * p1 + (1 - self.pi) * p2)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single layer of a BBB network. In essence this is a normal fc layer, but the weights and biases\n",
    "# are drawn from a gaussian distribution and the corresponding means and variances are learned\n",
    "\n",
    "# This class also stores the prior and variational posterior of the weights and biases of the last\n",
    "# forward pass through the layer\n",
    "\n",
    "class BayesianLayer(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, weight_prior: GaussianMixture, bias_prior: GaussianMixture):\n",
    "        super().__init__()\n",
    "        self.in_features, self.out_features = in_features, out_features\n",
    "        self.weight_prior, self.bias_prior = weight_prior, bias_prior\n",
    "        self.log_prior, self.log_posterior = 0, 0\n",
    "\n",
    "        # Weights\n",
    "        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features).uniform_(-0.2, 0.2))\n",
    "        self.weight_rho = nn.Parameter(torch.Tensor(out_features, in_features).uniform_(-5, -4))\n",
    "        self.weight = GaussianParameters(self.weight_mu, self.weight_rho)\n",
    "\n",
    "        # Biases\n",
    "        self.bias_mu = nn.Parameter(torch.Tensor(out_features).uniform_(-0.2, 0.2))\n",
    "        self.bias_rho = nn.Parameter(torch.Tensor(out_features).uniform_(-5, -4))\n",
    "        self.bias = GaussianParameters(self.bias_mu, self.bias_rho)\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.training:\n",
    "            weight = self.weight.sample()\n",
    "            bias = self.bias.sample()\n",
    "            # Log probability of the drawn parameters given the prior\n",
    "            self.log_prior = self.weight_prior.log_prob(weight) + self.bias_prior.log_prob(bias)\n",
    "            # Log probability of the drawn parameters given the parameter distribution\n",
    "            self.log_posterior = self.weight.log_prob(weight) + self.bias.log_prob(bias)\n",
    "        else:\n",
    "            # If we are not in training mode use deterministic parameters (i.e. the means)\n",
    "            weight = self.weight.mu\n",
    "            bias = self.bias.mu\n",
    "        return F.relu(F.linear(input, weight, bias))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BBB network with three layers (and currently fixed input/hidden/output dimensions). Stores also\n",
    "# the prior and variational posterior of the last forward pass\n",
    "\n",
    "class BayesianNetwork(nn.Module):\n",
    "    def __init__(self, weight_prior: GaussianMixture, bias_prior: GaussianMixture):\n",
    "        super().__init__()\n",
    "        self.first = BayesianLayer(28*28, 400, weight_prior, bias_prior)\n",
    "        self.second = BayesianLayer(400, 400, weight_prior, bias_prior)\n",
    "        self.third = BayesianLayer(400, 10, weight_prior, bias_prior)\n",
    "    \n",
    "    @property\n",
    "    def log_prior(self):\n",
    "        return self.first.log_prior + self.second.log_prior + self.third.log_prior\n",
    "\n",
    "    @property\n",
    "    def log_posterior(self):\n",
    "        return self.first.log_posterior + self.second.log_posterior + self.third.log_posterior\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        input = self.first(input)\n",
    "        input = self.second(input)\n",
    "        input = self.third(input)\n",
    "        return F.log_softmax(input, dim=1) # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net: BayesianNetwork, optimizer: torch.optim.Optimizer, loader, batch_count, monte_carlo_samples=1):\n",
    "    net.train()\n",
    "    for i, (data, target) in enumerate(loader):\n",
    "        total_loss = 0\n",
    "        batch_size = data.shape[0]\n",
    "\n",
    "        data = torch.flatten(data, start_dim=1).to(device)\n",
    "        target = target.to(device)\n",
    "        net.zero_grad()\n",
    "        for sample, target in zip(data, target):\n",
    "            input = torch.unsqueeze(sample, 0)\n",
    "            target = torch.unsqueeze(target, 0)\n",
    "            output = net(input)\n",
    "            log_prior = net.log_prior\n",
    "            log_posterior = net.log_posterior\n",
    "            loss = (log_posterior - log_prior) / batch_count + F.nll_loss(output, target, reduction=\"sum\")\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Completed minibatch #{i}; loss is {total_loss / batch_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed minibatch #0; loss is -11.086641311645508\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Uni\\Bachelor\\Code\\BayesByBackprob.ipynb Cell 10'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Uni/Bachelor/Code/BayesByBackprob.ipynb#ch0000009?line=6'>7</a>\u001b[0m \u001b[39m#optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Uni/Bachelor/Code/BayesByBackprob.ipynb#ch0000009?line=7'>8</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(net\u001b[39m.\u001b[39mparameters())\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Uni/Bachelor/Code/BayesByBackprob.ipynb#ch0000009?line=8'>9</a>\u001b[0m train(net, optimizer, trainloader, \u001b[39mlen\u001b[39;49m(trainloader))\n",
      "\u001b[1;32mc:\\Uni\\Bachelor\\Code\\BayesByBackprob.ipynb Cell 9'\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(net, optimizer, loader, batch_count, monte_carlo_samples)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Uni/Bachelor/Code/BayesByBackprob.ipynb#ch0000008?line=16'>17</a>\u001b[0m     total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Uni/Bachelor/Code/BayesByBackprob.ipynb#ch0000008?line=17'>18</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Uni/Bachelor/Code/BayesByBackprob.ipynb#ch0000008?line=18'>19</a>\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Uni/Bachelor/Code/BayesByBackprob.ipynb#ch0000008?line=19'>20</a>\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m50\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Uni/Bachelor/Code/BayesByBackprob.ipynb#ch0000008?line=20'>21</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCompleted minibatch #\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m; loss is \u001b[39m\u001b[39m{\u001b[39;00mtotal_loss \u001b[39m/\u001b[39m batch_size\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\optim\\optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///~/anaconda3/envs/ml/lib/site-packages/torch/optim/optimizer.py?line=85'>86</a>\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m     <a href='file:///~/anaconda3/envs/ml/lib/site-packages/torch/optim/optimizer.py?line=86'>87</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m---> <a href='file:///~/anaconda3/envs/ml/lib/site-packages/torch/optim/optimizer.py?line=87'>88</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\autograd\\grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///~/anaconda3/envs/ml/lib/site-packages/torch/autograd/grad_mode.py?line=24'>25</a>\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     <a href='file:///~/anaconda3/envs/ml/lib/site-packages/torch/autograd/grad_mode.py?line=25'>26</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     <a href='file:///~/anaconda3/envs/ml/lib/site-packages/torch/autograd/grad_mode.py?line=26'>27</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m():\n\u001b[1;32m---> <a href='file:///~/anaconda3/envs/ml/lib/site-packages/torch/autograd/grad_mode.py?line=27'>28</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\optim\\adam.py:133\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/ml/lib/site-packages/torch/optim/adam.py?line=129'>130</a>\u001b[0m             \u001b[39m# record the step after step update\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/ml/lib/site-packages/torch/optim/adam.py?line=130'>131</a>\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> <a href='file:///~/anaconda3/envs/ml/lib/site-packages/torch/optim/adam.py?line=132'>133</a>\u001b[0m     F\u001b[39m.\u001b[39;49madam(params_with_grad,\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/ml/lib/site-packages/torch/optim/adam.py?line=133'>134</a>\u001b[0m            grads,\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/ml/lib/site-packages/torch/optim/adam.py?line=134'>135</a>\u001b[0m            exp_avgs,\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/ml/lib/site-packages/torch/optim/adam.py?line=135'>136</a>\u001b[0m            exp_avg_sqs,\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/ml/lib/site-packages/torch/optim/adam.py?line=136'>137</a>\u001b[0m            max_exp_avg_sqs,\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/ml/lib/site-packages/torch/optim/adam.py?line=137'>138</a>\u001b[0m            state_steps,\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/ml/lib/site-packages/torch/optim/adam.py?line=138'>139</a>\u001b[0m            amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/ml/lib/site-packages/torch/optim/adam.py?line=139'>140</a>\u001b[0m            beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/ml/lib/site-packages/torch/optim/adam.py?line=140'>141</a>\u001b[0m            beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/ml/lib/site-packages/torch/optim/adam.py?line=141'>142</a>\u001b[0m            lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/ml/lib/site-packages/torch/optim/adam.py?line=142'>143</a>\u001b[0m            weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/ml/lib/site-packages/torch/optim/adam.py?line=143'>144</a>\u001b[0m            eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m    <a href='file:///~/anaconda3/envs/ml/lib/site-packages/torch/optim/adam.py?line=144'>145</a>\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\optim\\_functional.py:94\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[0;32m     <a href='file:///~/anaconda3/envs/ml/lib/site-packages/torch/optim/_functional.py?line=91'>92</a>\u001b[0m     denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39msqrt(bias_correction2))\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m     <a href='file:///~/anaconda3/envs/ml/lib/site-packages/torch/optim/_functional.py?line=92'>93</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///~/anaconda3/envs/ml/lib/site-packages/torch/optim/_functional.py?line=93'>94</a>\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39;49m math\u001b[39m.\u001b[39;49msqrt(bias_correction2))\u001b[39m.\u001b[39;49madd_(eps)\n\u001b[0;32m     <a href='file:///~/anaconda3/envs/ml/lib/site-packages/torch/optim/_functional.py?line=95'>96</a>\u001b[0m step_size \u001b[39m=\u001b[39m lr \u001b[39m/\u001b[39m bias_correction1\n\u001b[0;32m     <a href='file:///~/anaconda3/envs/ml/lib/site-packages/torch/optim/_functional.py?line=97'>98</a>\u001b[0m param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pi = 0.5 # 0.25, 0.5, 0.75\n",
    "sigma1 = np.exp(-1) # 0, 1, 2\n",
    "sigma2 = np.exp(-7) # 6, 7, 8\n",
    "prior = GaussianMixture(pi, sigma1, sigma2)\n",
    "\n",
    "net = BayesianNetwork(prior, prior).to(device)\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "train(net, optimizer, trainloader, len(trainloader))\n",
    "#torch.save(net.state_dict(), \"models/bbb_mnist_adam.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9185999631881714\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    corrects = 0\n",
    "    samples = 0\n",
    "    for data, target in testloader:\n",
    "        data = torch.flatten(data, start_dim=1).to(device)\n",
    "        target = target.to(device)\n",
    "        outputs = net(data)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        corrects += ((preds - target) == 0).sum()\n",
    "        samples += len(data)\n",
    "    print(f\"Test accuracy: {corrects / samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9228999614715576\n"
     ]
    }
   ],
   "source": [
    "net2 = BayesianNetwork(prior, prior).to(device)\n",
    "net2.load_state_dict(torch.load(\"models/bbb_mnist_adam.pth\"))\n",
    "\n",
    "with torch.no_grad():\n",
    "    net2.train(False)\n",
    "    corrects = 0\n",
    "    samples = 0\n",
    "    for data, target in testloader:\n",
    "        data = torch.flatten(data, start_dim=1).to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        outputs = net2(data)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        corrects += ((preds - target) == 0).sum()\n",
    "        samples += len(data)\n",
    "    print(f\"Test accuracy: {corrects / samples}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96836cc28e460061bca45187f20cd83662f914d0b92933e0c7f2f0f17d1b293c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
