{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See https://github.com/nitarshan/bayes-by-backprop/blob/master/Weight%20Uncertainty%20in%20Neural%20Networks.ipynb\n",
    "# The network structure is basically copied, while the learning code has been written mostly by me\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision.transforms import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"No cuda device available; using the CPU\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "\n",
    "transform = transforms.Compose([\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize(0,5, 0.5)\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "# dataiter = iter(trainloader)\n",
    "# images, labels = dataiter.next()\n",
    "# imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multidimensional gaussian distribution; for sampling weights and biases of a nn layer\n",
    "\n",
    "class GaussianParameters:\n",
    "    def __init__(self, mu: torch.Tensor, rho: torch.Tensor):\n",
    "        self.mu, self.rho = mu, rho\n",
    "        self.distribution = torch.distributions.Normal(0, 1)\n",
    "\n",
    "    @property\n",
    "    def sigma(self):\n",
    "        return torch.log1p(torch.exp(self.rho))\n",
    "\n",
    "    def sample(self):\n",
    "        epsilon = self.distribution.sample(self.rho.size()).to(device)\n",
    "        return self.mu + self.sigma * epsilon\n",
    "\n",
    "    def log_prob(self, value):\n",
    "        return (-np.log(np.sqrt(2 * np.pi)) - torch.log(self.sigma) - ((value - self.mu) ** 2) / (2 * self.sigma ** 2)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted sum of two gaussian distributions\n",
    "\n",
    "class GaussianMixture:\n",
    "    def __init__(self, pi, sigma1, sigma2):\n",
    "        self.pi, self.sigma1, self.sigma2 = pi, sigma1, sigma2\n",
    "        self.gaussian1 = torch.distributions.Normal(0, sigma1)\n",
    "        self.gaussian2 = torch.distributions.Normal(0, sigma2)\n",
    "\n",
    "    def log_prob(self, value):\n",
    "        p1 = torch.exp(self.gaussian1.log_prob(value))\n",
    "        p2 = torch.exp(self.gaussian2.log_prob(value))\n",
    "        return torch.log((self.pi * p1 + (1 - self.pi) * p2)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single layer of a BBB network. In essence this is a normal fc layer, but the weights and biases\n",
    "# are drawn from a gaussian distribution and the corresponding means and variances are learned\n",
    "\n",
    "# This class also stores the prior and variational posterior of the weights and biases of the last\n",
    "# forward pass through the layer\n",
    "\n",
    "class BayesianLayer(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, weight_prior: GaussianMixture, bias_prior: GaussianMixture):\n",
    "        super().__init__()\n",
    "        self.in_features, self.out_features = in_features, out_features\n",
    "        self.weight_prior, self.bias_prior = weight_prior, bias_prior\n",
    "        self.log_prior, self.log_posterior = 0, 0\n",
    "\n",
    "        # Weights\n",
    "        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features).uniform_(-0.2, 0.2))\n",
    "        self.weight_rho = nn.Parameter(torch.Tensor(out_features, in_features).uniform_(-5, -4))\n",
    "        self.weight = GaussianParameters(self.weight_mu, self.weight_rho)\n",
    "\n",
    "        # Biases\n",
    "        self.bias_mu = nn.Parameter(torch.Tensor(out_features).uniform_(-0.2, 0.2))\n",
    "        self.bias_rho = nn.Parameter(torch.Tensor(out_features).uniform_(-5, -4))\n",
    "        self.bias = GaussianParameters(self.bias_mu, self.bias_rho)\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.training:\n",
    "            weight = self.weight.sample()\n",
    "            bias = self.bias.sample()\n",
    "            # Log probability of the drawn parameters given the prior\n",
    "            self.log_prior = self.weight_prior.log_prob(weight) + self.bias_prior.log_prob(bias)\n",
    "            # Log probability of the drawn parameters given the parameter distribution\n",
    "            self.log_posterior = self.weight.log_prob(weight) + self.bias.log_prob(bias)\n",
    "        else:\n",
    "            # If we are not in training mode use deterministic parameters (i.e. the means)\n",
    "            weight = self.weight.mu\n",
    "            bias = self.bias.mu\n",
    "        return F.relu(F.linear(input, weight, bias))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BBB network with three layers (and currently fixed input/hidden/output dimensions). Stores also\n",
    "# the prior and variational posterior of the last forward pass\n",
    "\n",
    "class BayesianNetwork(nn.Module):\n",
    "    def __init__(self, weight_prior: GaussianMixture, bias_prior: GaussianMixture):\n",
    "        super().__init__()\n",
    "        self.first = BayesianLayer(28*28, 400, weight_prior, bias_prior)\n",
    "        self.second = BayesianLayer(400, 400, weight_prior, bias_prior)\n",
    "        self.third = BayesianLayer(400, 10, weight_prior, bias_prior)\n",
    "    \n",
    "    @property\n",
    "    def log_prior(self):\n",
    "        return self.first.log_prior + self.second.log_prior + self.third.log_prior\n",
    "\n",
    "    @property\n",
    "    def log_posterior(self):\n",
    "        return self.first.log_posterior + self.second.log_posterior + self.third.log_posterior\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        input = self.first(input)\n",
    "        input = self.second(input)\n",
    "        input = self.third(input)\n",
    "        return F.log_softmax(input, dim=1) # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net: BayesianNetwork, optimizer: torch.optim.Optimizer, loader, batch_count, monte_carlo_samples=1):\n",
    "    net.train()\n",
    "    for i, (data, target) in enumerate(loader):\n",
    "        total_loss = 0\n",
    "        batch_size = data.shape[0]\n",
    "\n",
    "        data = torch.flatten(data, start_dim=1).to(device)\n",
    "        target = target.to(device)\n",
    "        net.zero_grad()\n",
    "        for sample, target in zip(data, target):\n",
    "            input = torch.unsqueeze(sample, 0)\n",
    "            target = torch.unsqueeze(target, 0)\n",
    "            output = net(input)\n",
    "            log_prior = net.log_prior\n",
    "            log_posterior = net.log_posterior\n",
    "            loss = (log_posterior - log_prior) / batch_count + F.nll_loss(output, target, reduction=\"sum\")\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Completed minibatch #{i}; loss is {total_loss / batch_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed minibatch #0; loss is 148.9354705810547\n",
      "Completed minibatch #50; loss is 135.48603210449218\n",
      "Completed minibatch #100; loss is 132.15176086425782\n",
      "Completed minibatch #150; loss is 128.81471252441406\n",
      "Completed minibatch #200; loss is 125.94064483642578\n",
      "Completed minibatch #250; loss is 123.04268951416016\n",
      "Completed minibatch #300; loss is 120.30123596191406\n",
      "Completed minibatch #350; loss is 117.74050598144531\n",
      "Completed minibatch #400; loss is 115.30002288818359\n",
      "Completed minibatch #450; loss is 113.00689544677735\n",
      "Completed minibatch #500; loss is 110.45084381103516\n",
      "Completed minibatch #550; loss is 108.70049591064453\n",
      "Completed minibatch #600; loss is 106.13995513916015\n",
      "Completed minibatch #650; loss is 104.4159164428711\n",
      "Completed minibatch #700; loss is 101.55565032958984\n",
      "Completed minibatch #750; loss is 100.04078216552735\n",
      "Completed minibatch #800; loss is 98.56752166748046\n",
      "Completed minibatch #850; loss is 97.44494323730468\n",
      "Completed minibatch #900; loss is 95.72170715332031\n",
      "Completed minibatch #950; loss is 93.2639144897461\n",
      "Completed minibatch #1000; loss is 91.8188262939453\n",
      "Completed minibatch #1050; loss is 90.80089263916015\n",
      "Completed minibatch #1100; loss is 89.91878356933594\n",
      "Completed minibatch #1150; loss is 88.97633514404296\n",
      "Completed minibatch #1200; loss is 87.19850006103516\n",
      "Completed minibatch #1250; loss is 86.0695587158203\n",
      "Completed minibatch #1300; loss is 82.885791015625\n",
      "Completed minibatch #1350; loss is 83.78359527587891\n",
      "Completed minibatch #1400; loss is 82.33923797607422\n",
      "Completed minibatch #1450; loss is 80.86271667480469\n",
      "Completed minibatch #1500; loss is 79.47739868164062\n",
      "Completed minibatch #1550; loss is 78.94585723876953\n",
      "Completed minibatch #1600; loss is 77.50727844238281\n",
      "Completed minibatch #1650; loss is 76.13942413330078\n",
      "Completed minibatch #1700; loss is 75.01043701171875\n",
      "Completed minibatch #1750; loss is 74.48689575195313\n",
      "Completed minibatch #1800; loss is 73.09579467773438\n",
      "Completed minibatch #1850; loss is 73.12022094726562\n",
      "Completed minibatch #1900; loss is 71.29893798828125\n",
      "Completed minibatch #1950; loss is 69.64196319580078\n",
      "Completed minibatch #2000; loss is 69.30588989257812\n",
      "Completed minibatch #2050; loss is 68.5367218017578\n",
      "Completed minibatch #2100; loss is 67.5159408569336\n",
      "Completed minibatch #2150; loss is 66.2456558227539\n",
      "Completed minibatch #2200; loss is 65.65741119384765\n",
      "Completed minibatch #2250; loss is 65.98994140625\n",
      "Completed minibatch #2300; loss is 64.35659866333008\n",
      "Completed minibatch #2350; loss is 63.647945404052734\n",
      "Completed minibatch #2400; loss is 63.051473999023436\n",
      "Completed minibatch #2450; loss is 61.2528678894043\n",
      "Completed minibatch #2500; loss is 61.196764373779295\n",
      "Completed minibatch #2550; loss is 59.572173309326175\n",
      "Completed minibatch #2600; loss is 58.9447639465332\n",
      "Completed minibatch #2650; loss is 58.45742340087891\n",
      "Completed minibatch #2700; loss is 58.30025100708008\n",
      "Completed minibatch #2750; loss is 57.14222412109375\n",
      "Completed minibatch #2800; loss is 56.57559356689453\n",
      "Completed minibatch #2850; loss is 56.00433578491211\n",
      "Completed minibatch #2900; loss is 55.37138061523437\n",
      "Completed minibatch #2950; loss is 55.28401184082031\n",
      "Completed minibatch #3000; loss is 54.05447998046875\n",
      "Completed minibatch #3050; loss is 53.207740783691406\n",
      "Completed minibatch #3100; loss is 53.350057983398436\n",
      "Completed minibatch #3150; loss is 52.1459602355957\n",
      "Completed minibatch #3200; loss is 53.35045471191406\n",
      "Completed minibatch #3250; loss is 50.792748260498044\n",
      "Completed minibatch #3300; loss is 50.25902252197265\n",
      "Completed minibatch #3350; loss is 51.59603500366211\n",
      "Completed minibatch #3400; loss is 49.127923583984376\n",
      "Completed minibatch #3450; loss is 48.54721221923828\n",
      "Completed minibatch #3500; loss is 48.25677337646484\n",
      "Completed minibatch #3550; loss is 47.58570938110351\n",
      "Completed minibatch #3600; loss is 47.74134979248047\n",
      "Completed minibatch #3650; loss is 47.183939361572264\n",
      "Completed minibatch #3700; loss is 46.21008148193359\n",
      "Completed minibatch #3750; loss is 47.087267303466795\n",
      "Completed minibatch #3800; loss is 46.289974975585935\n",
      "Completed minibatch #3850; loss is 45.55233459472656\n",
      "Completed minibatch #3900; loss is 44.62904815673828\n",
      "Completed minibatch #3950; loss is 44.088016510009766\n",
      "Completed minibatch #4000; loss is 43.6723991394043\n",
      "Completed minibatch #4050; loss is 45.10740966796875\n",
      "Completed minibatch #4100; loss is 44.2496826171875\n",
      "Completed minibatch #4150; loss is 42.72410430908203\n",
      "Completed minibatch #4200; loss is 42.8084602355957\n",
      "Completed minibatch #4250; loss is 43.32951278686524\n",
      "Completed minibatch #4300; loss is 43.782981872558594\n",
      "Completed minibatch #4350; loss is 42.72639389038086\n",
      "Completed minibatch #4400; loss is 40.86232376098633\n",
      "Completed minibatch #4450; loss is 42.61878890991211\n",
      "Completed minibatch #4500; loss is 40.686144256591795\n",
      "Completed minibatch #4550; loss is 40.035517883300784\n",
      "Completed minibatch #4600; loss is 41.11970748901367\n",
      "Completed minibatch #4650; loss is 40.138912963867185\n",
      "Completed minibatch #4700; loss is 41.82658309936524\n",
      "Completed minibatch #4750; loss is 39.10792007446289\n",
      "Completed minibatch #4800; loss is 38.755819702148436\n",
      "Completed minibatch #4850; loss is 39.73673324584961\n",
      "Completed minibatch #4900; loss is 38.440520477294925\n",
      "Completed minibatch #4950; loss is 37.95464172363281\n",
      "Completed minibatch #5000; loss is 38.249595642089844\n",
      "Completed minibatch #5050; loss is 37.487246704101565\n",
      "Completed minibatch #5100; loss is 38.604610443115234\n",
      "Completed minibatch #5150; loss is 37.13916015625\n",
      "Completed minibatch #5200; loss is 37.45350646972656\n",
      "Completed minibatch #5250; loss is 36.86590728759766\n",
      "Completed minibatch #5300; loss is 36.92940216064453\n",
      "Completed minibatch #5350; loss is 37.09934616088867\n",
      "Completed minibatch #5400; loss is 36.631185913085936\n",
      "Completed minibatch #5450; loss is 37.127194213867185\n",
      "Completed minibatch #5500; loss is 36.51158599853515\n",
      "Completed minibatch #5550; loss is 35.69763259887695\n",
      "Completed minibatch #5600; loss is 36.83086929321289\n",
      "Completed minibatch #5650; loss is 35.948115539550784\n",
      "Completed minibatch #5700; loss is 36.12308807373047\n",
      "Completed minibatch #5750; loss is 36.92628707885742\n",
      "Completed minibatch #5800; loss is 35.539083862304686\n",
      "Completed minibatch #5850; loss is 35.071014404296875\n",
      "Completed minibatch #5900; loss is 34.684513092041016\n",
      "Completed minibatch #5950; loss is 36.132897186279294\n",
      "Completed minibatch #6000; loss is 35.25403594970703\n",
      "Completed minibatch #6050; loss is 34.60937423706055\n",
      "Completed minibatch #6100; loss is 34.194943237304685\n",
      "Completed minibatch #6150; loss is 34.69571762084961\n",
      "Completed minibatch #6200; loss is 33.94725799560547\n",
      "Completed minibatch #6250; loss is 33.83823852539062\n",
      "Completed minibatch #6300; loss is 34.373409271240234\n",
      "Completed minibatch #6350; loss is 33.63575057983398\n",
      "Completed minibatch #6400; loss is 34.344496154785155\n",
      "Completed minibatch #6450; loss is 33.45541763305664\n",
      "Completed minibatch #6500; loss is 33.68325347900391\n",
      "Completed minibatch #6550; loss is 33.577161407470705\n",
      "Completed minibatch #6600; loss is 33.94736938476562\n",
      "Completed minibatch #6650; loss is 33.20440521240234\n",
      "Completed minibatch #6700; loss is 33.9619743347168\n",
      "Completed minibatch #6750; loss is 33.423712158203124\n",
      "Completed minibatch #6800; loss is 33.2361198425293\n",
      "Completed minibatch #6850; loss is 32.81567535400391\n",
      "Completed minibatch #6900; loss is 32.73572311401367\n",
      "Completed minibatch #6950; loss is 33.34000244140625\n",
      "Completed minibatch #7000; loss is 33.677275848388675\n",
      "Completed minibatch #7050; loss is 32.520703125\n",
      "Completed minibatch #7100; loss is 32.46026382446289\n",
      "Completed minibatch #7150; loss is 32.973565673828126\n",
      "Completed minibatch #7200; loss is 32.343711853027344\n",
      "Completed minibatch #7250; loss is 33.87191390991211\n",
      "Completed minibatch #7300; loss is 32.49412307739258\n",
      "Completed minibatch #7350; loss is 33.536865234375\n",
      "Completed minibatch #7400; loss is 32.40114517211914\n",
      "Completed minibatch #7450; loss is 33.32936401367188\n",
      "Completed minibatch #7500; loss is 33.19712448120117\n",
      "Completed minibatch #7550; loss is 32.336268997192384\n",
      "Completed minibatch #7600; loss is 31.999718856811523\n",
      "Completed minibatch #7650; loss is 34.00185279846191\n",
      "Completed minibatch #7700; loss is 32.182315826416016\n",
      "Completed minibatch #7750; loss is 31.807859039306642\n",
      "Completed minibatch #7800; loss is 31.75392875671387\n",
      "Completed minibatch #7850; loss is 32.12942276000977\n",
      "Completed minibatch #7900; loss is 32.36080322265625\n",
      "Completed minibatch #7950; loss is 31.649913024902343\n",
      "Completed minibatch #8000; loss is 31.707077026367188\n",
      "Completed minibatch #8050; loss is 31.66801986694336\n",
      "Completed minibatch #8100; loss is 31.72257423400879\n",
      "Completed minibatch #8150; loss is 31.529741287231445\n",
      "Completed minibatch #8200; loss is 31.763712310791014\n",
      "Completed minibatch #8250; loss is 31.501406860351562\n",
      "Completed minibatch #8300; loss is 31.837884140014648\n",
      "Completed minibatch #8350; loss is 31.413790130615233\n",
      "Completed minibatch #8400; loss is 31.382424545288085\n",
      "Completed minibatch #8450; loss is 31.641506576538085\n",
      "Completed minibatch #8500; loss is 32.62961540222168\n",
      "Completed minibatch #8550; loss is 32.48553352355957\n",
      "Completed minibatch #8600; loss is 33.65270576477051\n",
      "Completed minibatch #8650; loss is 32.883703994750974\n",
      "Completed minibatch #8700; loss is 33.0184440612793\n",
      "Completed minibatch #8750; loss is 31.204576110839845\n",
      "Completed minibatch #8800; loss is 32.283897399902344\n",
      "Completed minibatch #8850; loss is 32.01782913208008\n",
      "Completed minibatch #8900; loss is 33.391306686401364\n",
      "Completed minibatch #8950; loss is 31.51527328491211\n",
      "Completed minibatch #9000; loss is 33.277300643920896\n",
      "Completed minibatch #9050; loss is 31.061924362182616\n",
      "Completed minibatch #9100; loss is 31.533288955688477\n",
      "Completed minibatch #9150; loss is 31.13714256286621\n",
      "Completed minibatch #9200; loss is 31.019550704956053\n",
      "Completed minibatch #9250; loss is 31.04079704284668\n",
      "Completed minibatch #9300; loss is 31.395265960693358\n",
      "Completed minibatch #9350; loss is 30.992877197265624\n",
      "Completed minibatch #9400; loss is 31.717633056640626\n",
      "Completed minibatch #9450; loss is 33.38360328674317\n",
      "Completed minibatch #9500; loss is 31.31382064819336\n",
      "Completed minibatch #9550; loss is 31.557353973388672\n",
      "Completed minibatch #9600; loss is 31.15244140625\n",
      "Completed minibatch #9650; loss is 30.99128074645996\n",
      "Completed minibatch #9700; loss is 31.372200393676756\n",
      "Completed minibatch #9750; loss is 31.10863800048828\n",
      "Completed minibatch #9800; loss is 32.46778869628906\n",
      "Completed minibatch #9850; loss is 32.08037986755371\n",
      "Completed minibatch #9900; loss is 31.37694892883301\n",
      "Completed minibatch #9950; loss is 31.498366928100587\n",
      "Completed minibatch #10000; loss is 31.585926055908203\n",
      "Completed minibatch #10050; loss is 30.753931045532227\n",
      "Completed minibatch #10100; loss is 30.85493049621582\n",
      "Completed minibatch #10150; loss is 31.289500045776368\n",
      "Completed minibatch #10200; loss is 31.11595687866211\n",
      "Completed minibatch #10250; loss is 31.896160888671876\n",
      "Completed minibatch #10300; loss is 30.747517776489257\n",
      "Completed minibatch #10350; loss is 30.737021255493165\n",
      "Completed minibatch #10400; loss is 33.63953475952148\n",
      "Completed minibatch #10450; loss is 30.783488082885743\n",
      "Completed minibatch #10500; loss is 30.79218521118164\n",
      "Completed minibatch #10550; loss is 32.438850021362306\n",
      "Completed minibatch #10600; loss is 31.299117279052734\n",
      "Completed minibatch #10650; loss is 33.986148834228516\n",
      "Completed minibatch #10700; loss is 31.39143486022949\n",
      "Completed minibatch #10750; loss is 32.860497665405276\n",
      "Completed minibatch #10800; loss is 31.13410530090332\n",
      "Completed minibatch #10850; loss is 30.958594512939452\n",
      "Completed minibatch #10900; loss is 31.128310012817384\n",
      "Completed minibatch #10950; loss is 31.818095016479493\n",
      "Completed minibatch #11000; loss is 31.2963565826416\n",
      "Completed minibatch #11050; loss is 31.128670120239256\n",
      "Completed minibatch #11100; loss is 30.794497299194337\n",
      "Completed minibatch #11150; loss is 30.639513397216795\n",
      "Completed minibatch #11200; loss is 32.59474830627441\n",
      "Completed minibatch #11250; loss is 32.06072769165039\n",
      "Completed minibatch #11300; loss is 30.670014572143554\n",
      "Completed minibatch #11350; loss is 31.81774787902832\n",
      "Completed minibatch #11400; loss is 31.338887786865236\n",
      "Completed minibatch #11450; loss is 31.61760139465332\n",
      "Completed minibatch #11500; loss is 32.45076026916504\n",
      "Completed minibatch #11550; loss is 30.609351348876952\n",
      "Completed minibatch #11600; loss is 30.668254470825197\n",
      "Completed minibatch #11650; loss is 30.64760856628418\n",
      "Completed minibatch #11700; loss is 30.664897537231447\n",
      "Completed minibatch #11750; loss is 31.06434783935547\n",
      "Completed minibatch #11800; loss is 30.668591690063476\n",
      "Completed minibatch #11850; loss is 31.30104217529297\n",
      "Completed minibatch #11900; loss is 31.178649520874025\n",
      "Completed minibatch #11950; loss is 30.6456657409668\n"
     ]
    }
   ],
   "source": [
    "pi = 0.5 # 0.25, 0.5, 0.75\n",
    "sigma1 = np.exp(-1) # 0, 1, 2\n",
    "sigma2 = np.exp(-7) # 6, 7, 8\n",
    "prior = GaussianMixture(pi, sigma1, sigma2)\n",
    "\n",
    "net = BayesianNetwork(prior, prior).to(device)\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "train(net, optimizer, trainloader, len(trainloader))\n",
    "#torch.save(net.state_dict(), \"models/bbb_mnist_adam.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9185999631881714\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    corrects = 0\n",
    "    samples = 0\n",
    "    for data, target in testloader:\n",
    "        data = torch.flatten(data, start_dim=1).to(device)\n",
    "        target = target.to(device)\n",
    "        outputs = net(data)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        corrects += ((preds - target) == 0).sum()\n",
    "        samples += len(data)\n",
    "    print(f\"Test accuracy: {corrects / samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9228999614715576\n"
     ]
    }
   ],
   "source": [
    "net2 = BayesianNetwork(prior, prior).to(device)\n",
    "net2.load_state_dict(torch.load(\"models/bbb_mnist_adam.pth\"))\n",
    "\n",
    "with torch.no_grad():\n",
    "    net2.train(False)\n",
    "    corrects = 0\n",
    "    samples = 0\n",
    "    for data, target in testloader:\n",
    "        data = torch.flatten(data, start_dim=1).to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        outputs = net2(data)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        corrects += ((preds - target) == 0).sum()\n",
    "        samples += len(data)\n",
    "    print(f\"Test accuracy: {corrects / samples}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96836cc28e460061bca45187f20cd83662f914d0b92933e0c7f2f0f17d1b293c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
