{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flo/anaconda3/envs/bdl2/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from training.util import sgd, plot_losses, adam, lr_scheduler, wilson_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import experiments.base.cifar as cifar\n",
    "\n",
    "batch_size = 128\n",
    "trainloader = cifar.cifar10_trainloader(\"/mnt/d/Uni/Bachelorarbeit/linux/data/\", batch_size=batch_size, subsample=5000)\n",
    "testloader = cifar.cifar10_testloader(\"/mnt/d/Uni/Bachelorarbeit/linux/data/\", batch_size=batch_size)\n",
    "#stl_testloader = cifar.stl10_testloader(\"/mnt/d/Uni/Bachelorarbeit/linux/data/\", batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import experiments.base.cifar as cifar\n",
    "corrupted_testloader = cifar.cifar10_corrupted_testloader(\"/mnt/d/Uni/Bachelorarbeit/linux/data/\", intensity=4, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss 2.042579174041748\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 1: loss 1.7964861392974854\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 2: loss 1.6696503162384033\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 3: loss 1.594142198562622\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 4: loss 1.5125367641448975\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 5: loss 1.4249951839447021\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 6: loss 1.3835885524749756\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 7: loss 1.315809965133667\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 8: loss 1.317182183265686\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 9: loss 1.1725093126296997\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 10: loss 1.1314666271209717\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 11: loss 1.1289560794830322\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 12: loss 1.0504941940307617\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 13: loss 1.0604459047317505\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 14: loss 1.0708327293395996\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 15: loss 1.0588268041610718\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 16: loss 1.0450799465179443\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 17: loss 1.0466749668121338\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 18: loss 0.9204392433166504\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 19: loss 0.8597061038017273\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 20: loss 0.8323653936386108\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 21: loss 0.8200334310531616\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 22: loss 0.8297045826911926\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 23: loss 0.7797574996948242\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 24: loss 0.7619935870170593\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 25: loss 0.8038729429244995\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 26: loss 0.7546269297599792\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 27: loss 0.8140918016433716\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 28: loss 0.7328335046768188\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 29: loss 0.7044548392295837\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 30: loss 0.8408552408218384\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 31: loss 0.8014752268791199\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 32: loss 0.688275158405304\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 33: loss 0.6896610260009766\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 34: loss 0.8494860529899597\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 35: loss 0.6893947720527649\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 36: loss 0.738915741443634\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 37: loss 0.7950359582901001\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 38: loss 0.6438571214675903\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 39: loss 0.6683329343795776\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 40: loss 0.6378000378608704\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 41: loss 0.6045514345169067\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 42: loss 0.6079768538475037\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 43: loss 0.5496261715888977\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 44: loss 0.5481130480766296\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 45: loss 0.5598745346069336\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 46: loss 0.6344351172447205\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 47: loss 0.6673071980476379\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 48: loss 0.654183030128479\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 49: loss 0.6003171801567078\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 50: loss 0.5189510583877563\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 51: loss 0.5627806186676025\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 52: loss 0.5778311491012573\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 53: loss 0.5802226066589355\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 54: loss 0.5319100022315979\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 55: loss 0.5483306646347046\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 56: loss 0.525123655796051\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 57: loss 0.4973369538784027\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 58: loss 0.42355531454086304\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 59: loss 0.4098131060600281\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 60: loss 0.42528900504112244\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 61: loss 0.41072478890419006\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 62: loss 0.45665401220321655\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 63: loss 0.5247166156768799\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 64: loss 0.5512346029281616\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 65: loss 0.4367808401584625\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 66: loss 0.6746162176132202\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 67: loss 0.5041778087615967\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 68: loss 0.4226970076560974\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 69: loss 0.42923060059547424\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 70: loss 0.39731717109680176\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 71: loss 0.4526635706424713\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 72: loss 0.5620034337043762\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 73: loss 0.4355899691581726\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 74: loss 0.4421601891517639\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 75: loss 0.3895531892776489\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 76: loss 0.455129474401474\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 77: loss 0.5135703086853027\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 78: loss 0.6530328989028931\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 79: loss 0.539238452911377\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 80: loss 0.4258211553096771\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 81: loss 0.37079334259033203\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 82: loss 0.34014803171157837\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 83: loss 0.29453244805336\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 84: loss 0.3754620850086212\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 85: loss 0.3411588966846466\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 86: loss 0.3004602789878845\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 87: loss 0.38013169169425964\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 88: loss 0.5316070318222046\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 89: loss 0.35524752736091614\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 90: loss 0.2593822479248047\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 91: loss 0.2396891862154007\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 92: loss 0.2645353078842163\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 93: loss 0.26803427934646606\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 94: loss 0.28503137826919556\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 95: loss 0.23162922263145447\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 96: loss 0.2257978916168213\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 97: loss 0.2594018876552582\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 98: loss 0.3679726719856262\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 99: loss 0.23721127212047577\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 100: loss 0.22202913463115692\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 101: loss 0.1647941619157791\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 102: loss 0.28959110379219055\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 103: loss 0.1814386397600174\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 104: loss 0.3038625717163086\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 105: loss 0.36527901887893677\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 106: loss 0.2659018039703369\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 107: loss 0.28743976354599\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 108: loss 0.26790642738342285\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 109: loss 0.24091419577598572\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 110: loss 0.19561456143856049\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 111: loss 0.21984247863292694\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 112: loss 0.15177401900291443\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 113: loss 0.15899793803691864\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 114: loss 0.1804303526878357\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 115: loss 0.24477124214172363\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 116: loss 0.19241473078727722\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 117: loss 0.25780829787254333\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 118: loss 0.2874196171760559\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 119: loss 0.17350511252880096\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 120: loss 0.10892565548419952\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 121: loss 0.19557256996631622\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 122: loss 0.1188066378235817\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 123: loss 0.15799033641815186\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 124: loss 0.12828175723552704\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 125: loss 0.254117876291275\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 126: loss 0.15168055891990662\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 127: loss 0.10615788400173187\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 128: loss 0.10269127041101456\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 129: loss 0.1606186181306839\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 130: loss 0.11182361841201782\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 131: loss 0.08102329075336456\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 132: loss 0.10281895101070404\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 133: loss 0.12453384697437286\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 134: loss 0.06420870125293732\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 135: loss 0.05214904621243477\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 136: loss 0.04557044059038162\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 137: loss 0.05310557037591934\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 138: loss 0.06227891519665718\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 139: loss 0.04638534039258957\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 140: loss 0.036570481956005096\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 141: loss 0.03086887300014496\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 142: loss 0.034419141709804535\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 143: loss 0.05114893987774849\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 144: loss 0.03652118146419525\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 145: loss 0.03515797108411789\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 146: loss 0.04978315159678459\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 147: loss 0.05773749202489853\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 148: loss 0.06593587249517441\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 149: loss 0.04290951415896416\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 150: loss 0.04573362320661545\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 151: loss 0.07594762742519379\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 152: loss 0.05512653663754463\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 153: loss 0.041278690099716187\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 154: loss 0.029857009649276733\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 155: loss 0.028755074366927147\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 156: loss 0.021877828985452652\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 157: loss 0.021110037341713905\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 158: loss 0.03140033036470413\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 159: loss 0.03601633384823799\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 160: loss 0.024443084374070168\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 161: loss 0.02151668630540371\n",
      "SWAG: Collected 0 out of 30 deviation samples and 0 out of 100 parameter samples\n",
      "Epoch 162: loss 0.025140468031167984\n",
      "SWAG: Collected 1 out of 30 deviation samples and 1 out of 100 parameter samples\n",
      "Epoch 163: loss 0.029129456728696823\n",
      "SWAG: Collected 2 out of 30 deviation samples and 2 out of 100 parameter samples\n",
      "Epoch 164: loss 0.03701237216591835\n",
      "SWAG: Collected 2 out of 30 deviation samples and 2 out of 100 parameter samples\n",
      "Epoch 165: loss 0.02723882719874382\n",
      "SWAG: Collected 3 out of 30 deviation samples and 3 out of 100 parameter samples\n",
      "Epoch 166: loss 0.034794457256793976\n",
      "SWAG: Collected 4 out of 30 deviation samples and 4 out of 100 parameter samples\n",
      "Epoch 167: loss 0.040198542177677155\n",
      "SWAG: Collected 5 out of 30 deviation samples and 5 out of 100 parameter samples\n",
      "Epoch 168: loss 0.02057557739317417\n",
      "SWAG: Collected 5 out of 30 deviation samples and 5 out of 100 parameter samples\n",
      "Epoch 169: loss 0.025395477190613747\n",
      "SWAG: Collected 6 out of 30 deviation samples and 6 out of 100 parameter samples\n",
      "Epoch 170: loss 0.05736341327428818\n",
      "SWAG: Collected 7 out of 30 deviation samples and 7 out of 100 parameter samples\n",
      "Epoch 171: loss 0.040329091250896454\n",
      "SWAG: Collected 7 out of 30 deviation samples and 7 out of 100 parameter samples\n",
      "Epoch 172: loss 0.0292514618486166\n",
      "SWAG: Collected 8 out of 30 deviation samples and 8 out of 100 parameter samples\n",
      "Epoch 173: loss 0.03277648612856865\n",
      "SWAG: Collected 9 out of 30 deviation samples and 9 out of 100 parameter samples\n",
      "Epoch 174: loss 0.021257933229207993\n",
      "SWAG: Collected 10 out of 30 deviation samples and 10 out of 100 parameter samples\n",
      "Epoch 175: loss 0.018192213028669357\n",
      "SWAG: Collected 10 out of 30 deviation samples and 10 out of 100 parameter samples\n",
      "Epoch 176: loss 0.019267115741968155\n",
      "SWAG: Collected 11 out of 30 deviation samples and 11 out of 100 parameter samples\n",
      "Epoch 177: loss 0.01585550233721733\n",
      "SWAG: Collected 12 out of 30 deviation samples and 12 out of 100 parameter samples\n",
      "Epoch 178: loss 0.01763865351676941\n",
      "SWAG: Collected 12 out of 30 deviation samples and 12 out of 100 parameter samples\n",
      "Epoch 179: loss 0.02710413932800293\n",
      "SWAG: Collected 13 out of 30 deviation samples and 13 out of 100 parameter samples\n",
      "Epoch 180: loss 0.0404086709022522\n",
      "SWAG: Collected 14 out of 30 deviation samples and 14 out of 100 parameter samples\n",
      "Epoch 181: loss 0.028580661863088608\n",
      "SWAG: Collected 15 out of 30 deviation samples and 15 out of 100 parameter samples\n",
      "Epoch 182: loss 0.033882446587085724\n",
      "SWAG: Collected 15 out of 30 deviation samples and 15 out of 100 parameter samples\n",
      "Epoch 183: loss 0.048877134919166565\n",
      "SWAG: Collected 16 out of 30 deviation samples and 16 out of 100 parameter samples\n",
      "Epoch 184: loss 0.029639309272170067\n",
      "SWAG: Collected 17 out of 30 deviation samples and 17 out of 100 parameter samples\n",
      "Epoch 185: loss 0.03359539806842804\n",
      "SWAG: Collected 17 out of 30 deviation samples and 17 out of 100 parameter samples\n",
      "Epoch 186: loss 0.04433856159448624\n",
      "SWAG: Collected 18 out of 30 deviation samples and 18 out of 100 parameter samples\n",
      "Epoch 187: loss 0.02473444864153862\n",
      "SWAG: Collected 19 out of 30 deviation samples and 19 out of 100 parameter samples\n",
      "Epoch 188: loss 0.04477987810969353\n",
      "SWAG: Collected 20 out of 30 deviation samples and 20 out of 100 parameter samples\n",
      "Epoch 189: loss 0.0624309703707695\n",
      "SWAG: Collected 20 out of 30 deviation samples and 20 out of 100 parameter samples\n",
      "Epoch 190: loss 0.06620511412620544\n",
      "SWAG: Collected 21 out of 30 deviation samples and 21 out of 100 parameter samples\n",
      "Epoch 191: loss 0.030040090903639793\n",
      "SWAG: Collected 22 out of 30 deviation samples and 22 out of 100 parameter samples\n",
      "Epoch 192: loss 0.025923023000359535\n",
      "SWAG: Collected 22 out of 30 deviation samples and 22 out of 100 parameter samples\n",
      "Epoch 193: loss 0.031227970495820045\n",
      "SWAG: Collected 23 out of 30 deviation samples and 23 out of 100 parameter samples\n",
      "Epoch 194: loss 0.06723223626613617\n",
      "SWAG: Collected 24 out of 30 deviation samples and 24 out of 100 parameter samples\n",
      "Epoch 195: loss 0.04507697746157646\n",
      "SWAG: Collected 25 out of 30 deviation samples and 25 out of 100 parameter samples\n",
      "Epoch 196: loss 0.04804512485861778\n",
      "SWAG: Collected 25 out of 30 deviation samples and 25 out of 100 parameter samples\n",
      "Epoch 197: loss 0.029743006452918053\n",
      "SWAG: Collected 26 out of 30 deviation samples and 26 out of 100 parameter samples\n",
      "Epoch 198: loss 0.02981669269502163\n",
      "SWAG: Collected 27 out of 30 deviation samples and 27 out of 100 parameter samples\n",
      "Epoch 199: loss 0.02948906645178795\n",
      "SWAG: Collected 27 out of 30 deviation samples and 27 out of 100 parameter samples\n",
      "Epoch 200: loss 0.02301657758653164\n",
      "SWAG: Collected 28 out of 30 deviation samples and 28 out of 100 parameter samples\n",
      "Epoch 201: loss 0.023148607462644577\n",
      "SWAG: Collected 29 out of 30 deviation samples and 29 out of 100 parameter samples\n",
      "Epoch 202: loss 0.022584853693842888\n",
      "SWAG: Collected 30 out of 30 deviation samples and 30 out of 100 parameter samples\n",
      "Epoch 203: loss 0.017913471907377243\n",
      "SWAG: Collected 30 out of 30 deviation samples and 30 out of 100 parameter samples\n",
      "Epoch 204: loss 0.021613124758005142\n",
      "SWAG: Collected 30 out of 30 deviation samples and 31 out of 100 parameter samples\n",
      "Epoch 205: loss 0.028842920437455177\n",
      "SWAG: Collected 30 out of 30 deviation samples and 32 out of 100 parameter samples\n",
      "Epoch 206: loss 0.03039461374282837\n",
      "SWAG: Collected 30 out of 30 deviation samples and 32 out of 100 parameter samples\n",
      "Epoch 207: loss 0.05074978992342949\n",
      "SWAG: Collected 30 out of 30 deviation samples and 33 out of 100 parameter samples\n",
      "Epoch 208: loss 0.029588311910629272\n",
      "SWAG: Collected 30 out of 30 deviation samples and 34 out of 100 parameter samples\n",
      "Epoch 209: loss 0.02616211213171482\n",
      "SWAG: Collected 30 out of 30 deviation samples and 35 out of 100 parameter samples\n",
      "Epoch 210: loss 0.024011176079511642\n",
      "SWAG: Collected 30 out of 30 deviation samples and 35 out of 100 parameter samples\n",
      "Epoch 211: loss 0.022906901314854622\n",
      "SWAG: Collected 30 out of 30 deviation samples and 36 out of 100 parameter samples\n",
      "Epoch 212: loss 0.03349997475743294\n",
      "SWAG: Collected 30 out of 30 deviation samples and 37 out of 100 parameter samples\n",
      "Epoch 213: loss 0.03272461146116257\n",
      "SWAG: Collected 30 out of 30 deviation samples and 37 out of 100 parameter samples\n",
      "Epoch 214: loss 0.03496672213077545\n",
      "SWAG: Collected 30 out of 30 deviation samples and 38 out of 100 parameter samples\n",
      "Epoch 215: loss 0.04100095108151436\n",
      "SWAG: Collected 30 out of 30 deviation samples and 39 out of 100 parameter samples\n",
      "Epoch 216: loss 0.04367637261748314\n",
      "SWAG: Collected 30 out of 30 deviation samples and 40 out of 100 parameter samples\n",
      "Epoch 217: loss 0.054609306156635284\n",
      "SWAG: Collected 30 out of 30 deviation samples and 40 out of 100 parameter samples\n",
      "Epoch 218: loss 0.07401879131793976\n",
      "SWAG: Collected 30 out of 30 deviation samples and 41 out of 100 parameter samples\n",
      "Epoch 219: loss 0.09046110510826111\n",
      "SWAG: Collected 30 out of 30 deviation samples and 42 out of 100 parameter samples\n",
      "Epoch 220: loss 0.0299134012311697\n",
      "SWAG: Collected 30 out of 30 deviation samples and 42 out of 100 parameter samples\n",
      "Epoch 221: loss 0.028072530403733253\n",
      "SWAG: Collected 30 out of 30 deviation samples and 43 out of 100 parameter samples\n",
      "Epoch 222: loss 0.04536040872335434\n",
      "SWAG: Collected 30 out of 30 deviation samples and 44 out of 100 parameter samples\n",
      "Epoch 223: loss 0.029840189963579178\n",
      "SWAG: Collected 30 out of 30 deviation samples and 45 out of 100 parameter samples\n",
      "Epoch 224: loss 0.02722983993589878\n",
      "SWAG: Collected 30 out of 30 deviation samples and 45 out of 100 parameter samples\n",
      "Epoch 225: loss 0.02782205305993557\n",
      "SWAG: Collected 30 out of 30 deviation samples and 46 out of 100 parameter samples\n",
      "Epoch 226: loss 0.02486857958137989\n",
      "SWAG: Collected 30 out of 30 deviation samples and 47 out of 100 parameter samples\n",
      "Epoch 227: loss 0.023677144199609756\n",
      "SWAG: Collected 30 out of 30 deviation samples and 47 out of 100 parameter samples\n",
      "Epoch 228: loss 0.03818230330944061\n",
      "SWAG: Collected 30 out of 30 deviation samples and 48 out of 100 parameter samples\n",
      "Epoch 229: loss 0.015639688819646835\n",
      "SWAG: Collected 30 out of 30 deviation samples and 49 out of 100 parameter samples\n",
      "Epoch 230: loss 0.039002470672130585\n",
      "SWAG: Collected 30 out of 30 deviation samples and 50 out of 100 parameter samples\n",
      "Epoch 231: loss 0.07881087064743042\n",
      "SWAG: Collected 30 out of 30 deviation samples and 50 out of 100 parameter samples\n",
      "Epoch 232: loss 0.030339080840349197\n",
      "SWAG: Collected 30 out of 30 deviation samples and 51 out of 100 parameter samples\n",
      "Epoch 233: loss 0.03809719532728195\n",
      "SWAG: Collected 30 out of 30 deviation samples and 52 out of 100 parameter samples\n",
      "Epoch 234: loss 0.038819026201963425\n",
      "SWAG: Collected 30 out of 30 deviation samples and 52 out of 100 parameter samples\n",
      "Epoch 235: loss 0.05156741291284561\n",
      "SWAG: Collected 30 out of 30 deviation samples and 53 out of 100 parameter samples\n",
      "Epoch 236: loss 0.030868882313370705\n",
      "SWAG: Collected 30 out of 30 deviation samples and 54 out of 100 parameter samples\n",
      "Epoch 237: loss 0.04090886935591698\n",
      "SWAG: Collected 30 out of 30 deviation samples and 55 out of 100 parameter samples\n",
      "Epoch 238: loss 0.05594363063573837\n",
      "SWAG: Collected 30 out of 30 deviation samples and 55 out of 100 parameter samples\n",
      "Epoch 239: loss 0.06579093635082245\n",
      "SWAG: Collected 30 out of 30 deviation samples and 56 out of 100 parameter samples\n",
      "Epoch 240: loss 0.06939640641212463\n",
      "SWAG: Collected 30 out of 30 deviation samples and 57 out of 100 parameter samples\n",
      "Epoch 241: loss 0.03032298944890499\n",
      "SWAG: Collected 30 out of 30 deviation samples and 57 out of 100 parameter samples\n",
      "Epoch 242: loss 0.026899326592683792\n",
      "SWAG: Collected 30 out of 30 deviation samples and 58 out of 100 parameter samples\n",
      "Epoch 243: loss 0.036707695573568344\n",
      "SWAG: Collected 30 out of 30 deviation samples and 59 out of 100 parameter samples\n",
      "Epoch 244: loss 0.02358192391693592\n",
      "SWAG: Collected 30 out of 30 deviation samples and 60 out of 100 parameter samples\n",
      "Epoch 245: loss 0.01967867836356163\n",
      "SWAG: Collected 30 out of 30 deviation samples and 60 out of 100 parameter samples\n",
      "Epoch 246: loss 0.014458823017776012\n",
      "SWAG: Collected 30 out of 30 deviation samples and 61 out of 100 parameter samples\n",
      "Epoch 247: loss 0.019127625972032547\n",
      "SWAG: Collected 30 out of 30 deviation samples and 62 out of 100 parameter samples\n",
      "Epoch 248: loss 0.027801865711808205\n",
      "SWAG: Collected 30 out of 30 deviation samples and 62 out of 100 parameter samples\n",
      "Epoch 249: loss 0.03498295694589615\n",
      "SWAG: Collected 30 out of 30 deviation samples and 63 out of 100 parameter samples\n",
      "Epoch 250: loss 0.05246906355023384\n",
      "SWAG: Collected 30 out of 30 deviation samples and 64 out of 100 parameter samples\n",
      "Epoch 251: loss 0.08188003301620483\n",
      "SWAG: Collected 30 out of 30 deviation samples and 65 out of 100 parameter samples\n",
      "Epoch 252: loss 0.08415831625461578\n",
      "SWAG: Collected 30 out of 30 deviation samples and 65 out of 100 parameter samples\n",
      "Epoch 253: loss 0.026557669043540955\n",
      "SWAG: Collected 30 out of 30 deviation samples and 66 out of 100 parameter samples\n",
      "Epoch 254: loss 0.017840048298239708\n",
      "SWAG: Collected 30 out of 30 deviation samples and 67 out of 100 parameter samples\n",
      "Epoch 255: loss 0.024988943710923195\n",
      "SWAG: Collected 30 out of 30 deviation samples and 67 out of 100 parameter samples\n",
      "Epoch 256: loss 0.059351999312639236\n",
      "SWAG: Collected 30 out of 30 deviation samples and 68 out of 100 parameter samples\n",
      "Epoch 257: loss 0.04906275123357773\n",
      "SWAG: Collected 30 out of 30 deviation samples and 69 out of 100 parameter samples\n",
      "Epoch 258: loss 0.023292791098356247\n",
      "SWAG: Collected 30 out of 30 deviation samples and 70 out of 100 parameter samples\n",
      "Epoch 259: loss 0.06331553310155869\n",
      "SWAG: Collected 30 out of 30 deviation samples and 70 out of 100 parameter samples\n",
      "Epoch 260: loss 0.05615653470158577\n",
      "SWAG: Collected 30 out of 30 deviation samples and 71 out of 100 parameter samples\n",
      "Epoch 261: loss 0.03185123950242996\n",
      "SWAG: Collected 30 out of 30 deviation samples and 72 out of 100 parameter samples\n",
      "Epoch 262: loss 0.04050114378333092\n",
      "SWAG: Collected 30 out of 30 deviation samples and 72 out of 100 parameter samples\n",
      "Epoch 263: loss 0.04904552549123764\n",
      "SWAG: Collected 30 out of 30 deviation samples and 73 out of 100 parameter samples\n",
      "Epoch 264: loss 0.0327577218413353\n",
      "SWAG: Collected 30 out of 30 deviation samples and 74 out of 100 parameter samples\n",
      "Epoch 265: loss 0.030905267223715782\n",
      "SWAG: Collected 30 out of 30 deviation samples and 75 out of 100 parameter samples\n",
      "Epoch 266: loss 0.04208966717123985\n",
      "SWAG: Collected 30 out of 30 deviation samples and 75 out of 100 parameter samples\n",
      "Epoch 267: loss 0.022577136754989624\n",
      "SWAG: Collected 30 out of 30 deviation samples and 76 out of 100 parameter samples\n",
      "Epoch 268: loss 0.0333540216088295\n",
      "SWAG: Collected 30 out of 30 deviation samples and 77 out of 100 parameter samples\n",
      "Epoch 269: loss 0.05510667711496353\n",
      "SWAG: Collected 30 out of 30 deviation samples and 77 out of 100 parameter samples\n",
      "Epoch 270: loss 0.02660585381090641\n",
      "SWAG: Collected 30 out of 30 deviation samples and 78 out of 100 parameter samples\n",
      "Epoch 271: loss 0.021213289350271225\n",
      "SWAG: Collected 30 out of 30 deviation samples and 79 out of 100 parameter samples\n",
      "Epoch 272: loss 0.030345534905791283\n",
      "SWAG: Collected 30 out of 30 deviation samples and 80 out of 100 parameter samples\n",
      "Epoch 273: loss 0.02341698296368122\n",
      "SWAG: Collected 30 out of 30 deviation samples and 80 out of 100 parameter samples\n",
      "Epoch 274: loss 0.039124391973018646\n",
      "SWAG: Collected 30 out of 30 deviation samples and 81 out of 100 parameter samples\n",
      "Epoch 275: loss 0.03985453397035599\n",
      "SWAG: Collected 30 out of 30 deviation samples and 82 out of 100 parameter samples\n",
      "Epoch 276: loss 0.04273681342601776\n",
      "SWAG: Collected 30 out of 30 deviation samples and 82 out of 100 parameter samples\n",
      "Epoch 277: loss 0.0423210933804512\n",
      "SWAG: Collected 30 out of 30 deviation samples and 83 out of 100 parameter samples\n",
      "Epoch 278: loss 0.052073944360017776\n",
      "SWAG: Collected 30 out of 30 deviation samples and 84 out of 100 parameter samples\n",
      "Epoch 279: loss 0.07900889962911606\n",
      "SWAG: Collected 30 out of 30 deviation samples and 85 out of 100 parameter samples\n",
      "Epoch 280: loss 0.11618606001138687\n",
      "SWAG: Collected 30 out of 30 deviation samples and 85 out of 100 parameter samples\n",
      "Epoch 281: loss 0.07192070782184601\n",
      "SWAG: Collected 30 out of 30 deviation samples and 86 out of 100 parameter samples\n",
      "Epoch 282: loss 0.05518718436360359\n",
      "SWAG: Collected 30 out of 30 deviation samples and 87 out of 100 parameter samples\n",
      "Epoch 283: loss 0.06954018771648407\n",
      "SWAG: Collected 30 out of 30 deviation samples and 87 out of 100 parameter samples\n",
      "Epoch 284: loss 0.03865862637758255\n",
      "SWAG: Collected 30 out of 30 deviation samples and 88 out of 100 parameter samples\n",
      "Epoch 285: loss 0.037295080721378326\n",
      "SWAG: Collected 30 out of 30 deviation samples and 89 out of 100 parameter samples\n",
      "Epoch 286: loss 0.05891132354736328\n",
      "SWAG: Collected 30 out of 30 deviation samples and 90 out of 100 parameter samples\n",
      "Epoch 287: loss 0.030865225940942764\n",
      "SWAG: Collected 30 out of 30 deviation samples and 90 out of 100 parameter samples\n",
      "Epoch 288: loss 0.021662121638655663\n",
      "SWAG: Collected 30 out of 30 deviation samples and 91 out of 100 parameter samples\n",
      "Epoch 289: loss 0.016638880595564842\n",
      "SWAG: Collected 30 out of 30 deviation samples and 92 out of 100 parameter samples\n",
      "Epoch 290: loss 0.016406236216425896\n",
      "SWAG: Collected 30 out of 30 deviation samples and 92 out of 100 parameter samples\n",
      "Epoch 291: loss 0.018068429082632065\n",
      "SWAG: Collected 30 out of 30 deviation samples and 93 out of 100 parameter samples\n",
      "Epoch 292: loss 0.02245902456343174\n",
      "SWAG: Collected 30 out of 30 deviation samples and 94 out of 100 parameter samples\n",
      "Epoch 293: loss 0.015194637700915337\n",
      "SWAG: Collected 30 out of 30 deviation samples and 95 out of 100 parameter samples\n",
      "Epoch 294: loss 0.0118633434176445\n",
      "SWAG: Collected 30 out of 30 deviation samples and 95 out of 100 parameter samples\n",
      "Epoch 295: loss 0.014899766072630882\n",
      "SWAG: Collected 30 out of 30 deviation samples and 96 out of 100 parameter samples\n",
      "Epoch 296: loss 0.03281013295054436\n",
      "SWAG: Collected 30 out of 30 deviation samples and 97 out of 100 parameter samples\n",
      "Epoch 297: loss 0.039496686309576035\n",
      "SWAG: Collected 30 out of 30 deviation samples and 97 out of 100 parameter samples\n",
      "Epoch 298: loss 0.015999168157577515\n",
      "SWAG: Collected 30 out of 30 deviation samples and 98 out of 100 parameter samples\n",
      "Epoch 299: loss 0.012196286581456661\n",
      "SWAG: Collected 30 out of 30 deviation samples and 99 out of 100 parameter samples\n",
      "Final loss 0.012196286581456661\n",
      "SWAG: Collected 30 out of 30 deviation samples and 99 out of 100 parameter samples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SwagModel(\n",
       "  (model): Sequential(\n",
       "    (0): PreResNet(\n",
       "      (model): Sequential(\n",
       "        (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): PreBasicBlock(\n",
       "          (main_path): Sequential(\n",
       "            (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (1): ReLU()\n",
       "            (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (4): ReLU()\n",
       "            (5): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (skip_path): Identity()\n",
       "        )\n",
       "        (2): PreBasicBlock(\n",
       "          (main_path): Sequential(\n",
       "            (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (1): ReLU()\n",
       "            (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (4): ReLU()\n",
       "            (5): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (skip_path): Identity()\n",
       "        )\n",
       "        (3): PreBasicBlock(\n",
       "          (main_path): Sequential(\n",
       "            (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (1): ReLU()\n",
       "            (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (4): ReLU()\n",
       "            (5): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (skip_path): Identity()\n",
       "        )\n",
       "        (4): PreBasicBlock(\n",
       "          (main_path): Sequential(\n",
       "            (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (1): ReLU()\n",
       "            (2): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (4): ReLU()\n",
       "            (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (skip_path): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        )\n",
       "        (5): PreBasicBlock(\n",
       "          (main_path): Sequential(\n",
       "            (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (1): ReLU()\n",
       "            (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (4): ReLU()\n",
       "            (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (skip_path): Identity()\n",
       "        )\n",
       "        (6): PreBasicBlock(\n",
       "          (main_path): Sequential(\n",
       "            (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (1): ReLU()\n",
       "            (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (4): ReLU()\n",
       "            (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (skip_path): Identity()\n",
       "        )\n",
       "        (7): PreBasicBlock(\n",
       "          (main_path): Sequential(\n",
       "            (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (1): ReLU()\n",
       "            (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (4): ReLU()\n",
       "            (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (skip_path): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        )\n",
       "        (8): PreBasicBlock(\n",
       "          (main_path): Sequential(\n",
       "            (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (1): ReLU()\n",
       "            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (4): ReLU()\n",
       "            (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (skip_path): Identity()\n",
       "        )\n",
       "        (9): PreBasicBlock(\n",
       "          (main_path): Sequential(\n",
       "            (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (1): ReLU()\n",
       "            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (4): ReLU()\n",
       "            (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (skip_path): Identity()\n",
       "        )\n",
       "        (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        (11): ReLU()\n",
       "        (12): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
       "        (13): Flatten(start_dim=1, end_dim=-1)\n",
       "        (14): Linear(in_features=64, out_features=10, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (1): LogSoftmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from training.pp import MAP\n",
    "from training.swag import SwagModel\n",
    "\n",
    "layers = [\n",
    "    (\"preresnet-20\", (32, 3, 10)),\n",
    "    (\"logsoftmax\", ())\n",
    "]\n",
    "\n",
    "config = {\n",
    "    \"deviation_samples\": 30,\n",
    "    \"mean_samples\": 100,\n",
    "    \"start_epoch\": 161\n",
    "}\n",
    "\n",
    "model = SwagModel(layers, config)\n",
    "\n",
    "model.train_model(300, torch.nn.NLLLoss(), sgd(1e-1, weight_decay=3e-4, momentum=0.9, nesterov=True), trainloader, batch_size, device, scheduler_factory=wilson_scheduler(161, 0.1, 0.01))\n",
    "#model.load_state_dict(torch.load(\"/mnt/d/Uni/Bachelorarbeit/results/CIFAR10/2/results/MAP/log/rep_00model.tar\"))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.7347999811172485\n",
      " Avg Log Likelihood: -1.2561177015304565\n",
      " Avg Likelihood: 0.6561229228973389\n",
      " ECE: 0.07758343744277954\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.7348),\n",
       " tensor(-1.2561),\n",
       " tensor(0.6561),\n",
       " <training.calibration.ClassificationCalibrationResults at 0x7fdf1ca64490>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import experiments.base.multiclass_classification as exp\n",
    "\n",
    "exp.eval_model(model, 5, testloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.bbb import BBBModel, GaussianPrior\n",
    "\n",
    "prior = GaussianPrior(torch.tensor(0), torch.tensor(1))\n",
    "layers = [\n",
    "    (\"variational-preresnet-20\", (32, 3, 10, prior)),\n",
    "    (\"logsoftmax\", ())\n",
    "]\n",
    "\n",
    "model = BBBModel(layers)\n",
    "\n",
    "model.train_model(120, torch.nn.NLLLoss(), sgd(1e-1), trainloader, batch_size, device, scheduler_factory=lr_scheduler([80, 120], 0.1), mc_samples=2, kl_rescaling=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5264)\n",
      " Accuracy: 0.37957367300987244\n",
      " Avg Log Likelihood: -2.0528364181518555\n",
      " Avg Likelihood: 0.30053094029426575\n",
      " ECE: 0.146800649462405\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.3796),\n",
       " tensor(-2.0528),\n",
       " tensor(0.3005),\n",
       " <training.calibration.ClassificationCalibrationResults at 0x7fcc00161f70>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import experiments.base.multiclass_classification as exp\n",
    "\n",
    "model.to(device)\n",
    "exp.eval_model(model, 5, corrupted_testloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.720300018787384\n",
      " Avg Log Likelihood: -1.1909916400909424\n",
      " Avg Likelihood: 0.6958616971969604\n",
      " ECE: 0.15854743110537528\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.7203),\n",
       " tensor(-1.1910),\n",
       " tensor(0.6959),\n",
       " <training.calibration.ClassificationCalibrationResults at 0x7f6c75a63c40>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import experiments.base.multiclass_classification as exp\n",
    "\n",
    "exp.eval_model(model, 5, testloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss 2.312675714492798\n",
      "Epoch 1: loss 2.3029868602752686\n",
      "Epoch 2: loss 2.302725315093994\n",
      "Epoch 3: loss 2.302553653717041\n",
      "Epoch 4: loss 2.3026626110076904\n",
      "Epoch 5: loss 2.302441120147705\n",
      "Epoch 6: loss 2.3030710220336914\n",
      "Epoch 7: loss 2.302055835723877\n",
      "Epoch 8: loss 2.303133010864258\n",
      "Epoch 9: loss 2.302640438079834\n",
      "Epoch 10: loss 2.3023521900177\n",
      "Epoch 11: loss 2.3024511337280273\n",
      "Epoch 12: loss 2.3020544052124023\n",
      "Epoch 13: loss 2.3023881912231445\n",
      "Epoch 14: loss 2.302034378051758\n",
      "Epoch 15: loss 2.302043914794922\n",
      "Epoch 16: loss 2.3025217056274414\n",
      "Epoch 17: loss 2.3022818565368652\n",
      "Epoch 18: loss 2.3018014430999756\n",
      "Epoch 19: loss 2.3017783164978027\n",
      "Epoch 20: loss 2.3025922775268555\n",
      "Epoch 21: loss 2.301764965057373\n",
      "Epoch 22: loss 2.301638126373291\n",
      "Epoch 23: loss 2.3018383979797363\n",
      "Epoch 24: loss 2.3023488521575928\n",
      "Epoch 25: loss 2.3016860485076904\n",
      "Epoch 26: loss 2.301741361618042\n",
      "Epoch 27: loss 2.301283836364746\n",
      "Epoch 28: loss 2.3019394874572754\n",
      "Epoch 29: loss 2.301410436630249\n",
      "Epoch 30: loss 2.301616668701172\n",
      "Epoch 31: loss 2.3017561435699463\n",
      "Epoch 32: loss 2.301504611968994\n",
      "Epoch 33: loss 2.3014166355133057\n",
      "Epoch 34: loss 2.301985263824463\n",
      "Epoch 35: loss 2.3015425205230713\n",
      "Epoch 36: loss 2.3018343448638916\n",
      "Epoch 37: loss 2.3012735843658447\n",
      "Epoch 38: loss 2.3017024993896484\n",
      "Epoch 39: loss 2.3012986183166504\n",
      "Epoch 40: loss 2.301623821258545\n",
      "Epoch 41: loss 2.301360845565796\n",
      "Epoch 42: loss 2.3009963035583496\n",
      "Epoch 43: loss 2.300992965698242\n",
      "Epoch 44: loss 2.301403522491455\n",
      "Epoch 45: loss 2.3007826805114746\n",
      "Epoch 46: loss 2.3012728691101074\n",
      "Epoch 47: loss 2.3008875846862793\n",
      "Epoch 48: loss 2.300393581390381\n",
      "Epoch 49: loss 2.2995760440826416\n",
      "Epoch 50: loss 2.3002007007598877\n",
      "Epoch 51: loss 2.2993083000183105\n",
      "Epoch 52: loss 2.2994585037231445\n",
      "Epoch 53: loss 2.2990939617156982\n",
      "Epoch 54: loss 2.2984113693237305\n",
      "Epoch 55: loss 2.2980692386627197\n",
      "Epoch 56: loss 2.2974212169647217\n",
      "Epoch 57: loss 2.296400547027588\n",
      "Epoch 58: loss 2.295881748199463\n",
      "Epoch 59: loss 2.2951507568359375\n",
      "Epoch 60: loss 2.2931129932403564\n",
      "Epoch 61: loss 2.2911534309387207\n",
      "Epoch 62: loss 2.288911819458008\n",
      "Epoch 63: loss 2.2866575717926025\n",
      "Epoch 64: loss 2.282620906829834\n",
      "Epoch 65: loss 2.2794671058654785\n",
      "Epoch 66: loss 2.274456024169922\n",
      "Epoch 67: loss 2.2669811248779297\n",
      "Epoch 68: loss 2.268989324569702\n",
      "Epoch 69: loss 2.2589240074157715\n",
      "Epoch 70: loss 2.2522411346435547\n",
      "Epoch 71: loss 2.2755894660949707\n",
      "Epoch 72: loss 2.2777841091156006\n",
      "Epoch 73: loss 2.2650070190429688\n",
      "Epoch 74: loss 2.2592310905456543\n",
      "Epoch 75: loss 2.249688148498535\n",
      "Epoch 76: loss 2.244169235229492\n",
      "Epoch 77: loss 2.2317657470703125\n",
      "Epoch 78: loss 2.2213993072509766\n",
      "Epoch 79: loss 2.2090094089508057\n",
      "Epoch 80: loss 2.232740879058838\n",
      "Epoch 81: loss 2.2051963806152344\n",
      "Epoch 82: loss 2.202758312225342\n",
      "Epoch 83: loss 2.194479465484619\n",
      "Epoch 84: loss 2.1794166564941406\n",
      "Epoch 85: loss 2.160524368286133\n",
      "Epoch 86: loss 2.1535074710845947\n",
      "Epoch 87: loss 2.1421639919281006\n",
      "Epoch 88: loss 2.130469560623169\n",
      "Epoch 89: loss 2.109483242034912\n",
      "Epoch 90: loss 2.1073806285858154\n",
      "Epoch 91: loss 2.092695474624634\n",
      "Epoch 92: loss 2.10007905960083\n",
      "Epoch 93: loss 2.0742883682250977\n",
      "Epoch 94: loss 2.107057571411133\n",
      "Epoch 95: loss 2.0735421180725098\n",
      "Epoch 96: loss 2.0465235710144043\n",
      "Epoch 97: loss 2.0446412563323975\n",
      "Epoch 98: loss 2.2152156829833984\n",
      "Epoch 99: loss 2.084442615509033\n",
      "Final loss 2.084442615509033\n"
     ]
    }
   ],
   "source": [
    "from training.pp import MAP\n",
    "from training.vogn import iVONModuleFunctorch, VOGNModule\n",
    "\n",
    "layers = [\n",
    "    (\"preresnet-20\", (32, 3, 10)),\n",
    "    (\"logsoftmax\", ())\n",
    "]\n",
    "\n",
    "model = VOGNModule(layers)\n",
    "model.train_model(100, torch.nn.NLLLoss(), {\"lr\": 1e-3, \"prior_prec\": 50, \"betas\": (0.9, 0.999), \"damping\": 1e-3, \"augmentation\": 1, \"sample\": False}, trainloader, batch_size, device, mc_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.10000000149011612\n",
      " Avg Log Likelihood: -2.3628106117248535\n",
      " Avg Likelihood: 0.10211975127458572\n",
      " ECE: 0.06623258305341005\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.1000),\n",
       " tensor(-2.3628),\n",
       " tensor(0.1021),\n",
       " <training.calibration.ClassificationCalibrationResults at 0x7fdf1a77d040>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import experiments.base.multiclass_classification as exp\n",
    "\n",
    "model.to(device)\n",
    "exp.eval_model(model, 5, testloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAwnklEQVR4nO3deXwV5fn//9eVBcK+BgwEDAgKyBIgIiq1Vm2rCAJFxd2qFW21otXW5fP7Wq22H2s/rtWquFsVBcEq4FJqXXBDE3YICipLIEAIshMg5Pr9cSYaYxJOSHImy/v5eJxHJjP3nHkfllyZ+565x9wdERGRgxUXdgAREanbVEhERKRKVEhERKRKVEhERKRKVEhERKRKEsIOEIb27dt7Wlpa2DFEROqUrKysTe6eXHp9gywkaWlpZGZmhh1DRKROMbNVZa1X15aIiFSJComIiFSJComIiFRJgxwjERE5WPv27SMnJ4eCgoKwo9SYpKQkUlNTSUxMjKq9ComISCXk5OTQokUL0tLSMLOw41Q7dyc/P5+cnBy6desW1T7q2hIRqYSCggLatWtXL4sIgJnRrl27Sp1xqZCIiFRSfS0ixSr7+VRIKmHe6m945L0vw44hIlKrqJBUwqvz13HnG8t4a8n6sKOISANmZlxwwQXffl9YWEhycjIjRoz4XrtRo0ZxzDHHfG/drbfeSufOnUlPT6dv37689tprVc6jQlIJNw3vxYDUVlw/eQGr8neGHUdEGqhmzZqxePFidu/eDcCsWbPo3Lnz99ps2bKFuXPnsmXLFr7++uvvbbv22muZP38+U6ZM4ZJLLqGoqKhKeVRIKqFxQjwPnTeIuDjjiufmUrBvf9iRRKSBOvXUU5k5cyYAkyZN4pxzzvne9qlTpzJy5EjOPvtsXnzxxTLfo3fv3iQkJLBp06YqZdHlv5WU2qYp941L5+KnP+OPry7hr2f0DzuSiITktulLWLpuW7W+Z59OLfnjyCMP2O7ss8/mT3/6EyNGjGDhwoVccsklzJ49+9vtkyZN4o9//CMdO3bkjDPO4KabbvrBe8yZM4e4uDiSk38wD2OlqJAchJ/06sBvT+zB3/+7gsFpbTgro0vYkUSkgenfvz8rV65k0qRJDB8+/HvbNmzYwIoVKxg2bBhmRkJCAosXL6Zv374A3HvvvTz33HO0aNGCl156qcpXoamQHKRrTj6cuau/4f/9azF9O7WiT6eWYUcSkRiL5syhJp1++ulcf/31vPvuu+Tn53+7/qWXXuKbb7759obCbdu28eKLL3LHHXcAkTGS66+/vtpyaIzkIMXHGfefPZDWTRP5zfNZbCvYF3YkEWlgLrnkEm655Rb69ev3vfWTJk3izTffZOXKlaxcuZKsrKxyx0mqQ8wKiZl1MbN3zCzbzJaY2YQy2owys4VmNt/MMs1sWKnt8WY2z8xmlFjX1sxmmdny4GubWHwegPbNG/PQuYPI+WY3v5+yAHeP1aFFREhNTWXChO//KF25ciWrV69m6NCh367r1q0bLVu2ZM6cOTWSw2L1w8/MUoAUd59rZi2ALGC0uy8t0aY5sNPd3cz6A5PdvVeJ7b8DMoCW7j4iWHcXsNnd7zSzG4E27n5DRVkyMjK8Oh9s9fjsr7hjZjb/M7w3lx3fvdreV0Rqn+zsbHr37h12jBpX1uc0syx3zyjdNmZnJO6e6+5zg+XtQDbQuVSbHf5dZWsGfFvlzCwVOA14vNRbjwKeCZafAUZXe/gDuHRYN07tewh3vrmMz1ZujvXhRURCFcoYiZmlAQOBH5xnmdkYM1sGzAQuKbHpPuAPQOk7Zzq6ey5EihXQoZxjjg+6yzLz8vKq/BlKvTd3ndGfrm2bcuXzc8nbvqda319EpDaLeSEJuq+mAte4+w8uwHb3V4LurNHA7cE+I4CN7p51sMd194nunuHuGVW9ZrosLZIS+cd5g9hWsI+rJ81jf5HGS0Tqq/o+HlrZzxfTQmJmiUSKyPPuPq2itu7+PnCYmbUHjgNON7OVwIvAiWb2XNB0QzD+UjwOs7Gm8h9I75SW3DG6Hx9/lc89sz4PK4aI1KCkpCTy8/PrbTEpfh5JUlJS1PvE7D4Si9zx8gSQ7e73lNOmB/BlMNg+CGgE5Lv7TcBNQZsTgOvd/fxgt9eAi4A7g6+v1uTnOJAzBqeSuXIzD73zJYO6tuGk3h3DjCMi1Sw1NZWcnByqu4u8Nil+QmK0YnlD4nHABcAiM5sfrLsZ6Arg7o8AY4ELzWwfsBsY5wcu+3cCk83sUmA1cGYNZK+UW08/kkVrt3LtS/OZefWP6NK2adiRRKSaJCYmRv3kwIYiZpf/1ibVfflvWVbn7+K0v88mrV0zplxxDEmJ8TV6PBGRmhb65b8NTdd2TbnnrHQWrd3K7TOWHngHEZE6SoWkBv20T0cu/3F3np+zmlfm5YQdR0SkRqiQ1LDf/+wIhnRry83TFvP5+u1hxxERqXYqJDUsIT6OB88ZSLPGCfz6+Sx27CkMO5KISLVSIYmBDi2TePDcgazctJMbpi6st9efi0jDpEISI0O7t+P3P+/FzIW5PP3RyrDjiIhUGxWSGLr8+O6c3Lsjf56ZTdaqb8KOIyJSLVRIYiguzrj7zAGktE7iqhfmkr9DkzuKSN2nQhJjrZom8vB5g8nfuZdrXpqvyR1FpM5TIQlB386tuO30I5m9fBMPvL087DgiIlWiQhKSs4/qwthBqTzw3+W890X9nfxNROo/FZKQmBl3jO7LER1bcM2L81i7ZXfYkUREDooKSYiaNIrnH+cNYt9+58rn57K3sPTDH0VEaj8VkpB1T27OXWf0Z/6aLfzl9eyw44iIVJoKSS0wvF8Klw7rxtMfrWT6gnVhxxERqRQVklrixlN7MfjQNtw4dSErNu4IO46ISNRUSGqJxPg4Hjp3EEmJ8fz6uSx27dXkjiJSN6iQ1CKHtEri/rMHsiJvBzdPW6TJHUWkTlAhqWWG9WzP704+nH/NX8fzc1aHHUdE5IBUSGqhK3/SgxOOSOZP05eyMGdL2HFERCqkQlILxcUZ956VTnKLxvz6ubls2bU37EgiIuWKWSExsy5m9o6ZZZvZEjObUEabUWa20Mzmm1mmmQ0L1ieZ2admtiDY97YS+9xqZmuDfeab2fBYfaaa1KZZI/5x3iA2bi/g2pfmU6TJHUWklorlGUkhcJ279waGAleaWZ9Sbd4GBrh7OnAJ8Hiwfg9worsPANKBU8xsaIn97nX39OD1ek1+iFga0KU1t4zowzuf5/Hwe1+GHUdEpEwxKyTunuvuc4Pl7UA20LlUmx3+3aVKzQAP1ru7F99ckRi8GsSv6OcPPZRR6Z24+9+f8+GKTWHHERH5gVDGSMwsDRgIzClj2xgzWwbMJHJWUrw+3szmAxuBWe5ect+rgi6xJ82sTTnHHB90l2Xm5dWd2XbNjL+M6Uf35OZMeHEe67cWhB1JROR7Yl5IzKw5MBW4xt23ld7u7q+4ey9gNHB7ifX7gy6vVGCImfUNNj0MHEakyysXuLus47r7RHfPcPeM5OTk6vtAMdCscQKPnD+IXXv3c9ULc9m3X5M7ikjtEdNCYmaJRIrI8+4+raK27v4+cJiZtS+1fgvwLnBK8P2GoMgUAY8BQ2ogeuh6dGjBnWP7k7nqG+56c1nYcUREvhXLq7YMeALIdvd7ymnTI2iHmQ0CGgH5ZpZsZq2D9U2Ak4FlwfcpJd5iDLC4xj5EyE4f0ImLjjmUx2Z/zZuLc8OOIyICQEIMj3UccAGwKBjrALgZ6Arg7o8AY4ELzWwfsBsY5+4eFItnzCyeSPGb7O4zgve4y8zSiQy+rwQuj83HCcfNp/VmQc5Wrpu8gO7JzTm8Y4uwI4lIA2cNcT6njIwMz8zMDDvGQcvdupuRf/+Q5o3jefXKYbRqmhh2JBFpAMwsy90zSq/Xne11UEqrJjx6wSDWbtnNVZPmsl83K4pIiFRI6qjBh7blT6P6Mnv5Jg2+i0ioYjlGItXsnCFdWbpuG4++/xV9OrVkVHrnA+8kIlLNdEZSx90ysg9DurXlDy8vZPHarWHHEZEGSIWkjkuMj+Mf5w2iXbNGjH82k0079oQdSUQaGBWSeqB988ZMvDCDzbv28pvn5rK3UHe+i0jsqJDUE307t+KvY/vz6crN/GnGkrDjiEgDosH2emRUeudvB9+P7NSKc4Z0DTuSiDQAOiOpZ/5wSi9+fHgyt7y6mMyVm8OOIyINwEEXkmBerKTqDCNVFx9nPHD2QDq3bsIVz80ld+vusCOJSD0XVSExs7+Y2UXBspnZLOALINfMjq7JgFJ5rZom8tiFGezeW8jl/8yiYN/+sCOJSD0W7RnJecDnwfKpRJ79MRR4Friz+mNJVfXs2IJ7x6WzMGcrN09bREOcU01EYiPaQtIRyAmWhxOZffdT4O9EnnQotdDPjjyEa08+nGnz1vLEB1+HHUdE6qloC0k+cGiw/DPgv8FyAmDVHUqqz29P7MHPj+zIX17P5oPleua7iFS/aAvJVOCFYGykLfBmsD4dWFEDuaSaxMUZd5+VTo8Ozblq0lxW5+8KO5KI1DPRFpLfAQ8AS4GfuvvOYH0KkWemSy3WvHECj12YgTtc9mwmO/cUhh1JROqRqAqJuxe6+93uPsHd55VYf6+7P15z8aS6HNquGQ+dO4jlG7dz/ZQFGnwXkWoT7eW/Py55ma+Z/dLMPjCzR82sec3Fk+o0rGd7bh7emzcWr+fB/6pHUkSqR7RdW/cBhwCY2RHAo8BC4BjgbzWSTGrEpcO6MWZgZ+6e9QWzlm4IO46I1APRFpLDgEXB8lhglrv/BrgMGFkTwaRmmBn/+4t+9E9txbUvzWfFxu1hRxKROi7aQuJAfLB8Et9dtbUeaFfdoaRmJSXG88j5g0lKjOOyZ7PYuntf2JFEpA6LtpB8Bvw/M7sA+BHwRrA+jUgxOSAz62Jm75hZtpktMbMJZbQZZWYLzWy+mWWa2bBgfZKZfWpmC4J9byuxT1szm2Vmy4OvbaL8TA1ap9ZNePj8weR8s4urJ81jf5EG30Xk4ERbSK4hcs/Ig8Cf3f3LYP2ZwEdRvkchcJ279yYyvcqVZtanVJu3gQHung5cAhRfEbYHONHdBwQ5TjGzocG2G4G33b1nsP+NUeZp8I5Ka8ttp/flvS/y+Ntbnx94BxGRMkT1PBJ3Xwz0L2PT9UBUMwK6ey6QGyxvN7NsoDORe1OK2+wosUszIl1qeORa1eJticGr+FfoUcAJwfIzwLvADdFkEjj36K4sWbeVR977kt4pLRiV3jnsSCJSx1RqGnkz625mI8zsNDPr7u4F7l7pDnYzSyMyR9ecMraNMbNlwEwiZyXF6+PNbD6wkchgf/G+HYMiVVysOpRzzPFBd1lmXl5eZSPXa38ceSRHpbXhhqkLWbx2a9hxRKSOifY+kpZmNoXIdCj/Al4FlpvZZDNrUZkDBvedTAWucfdtpbe7+yvu3gsYDdxeYv3+oMsrFRhiZn0rc1x3n+juGe6ekZycXJld671GCXH847zBtGnaiMv/mcWmHXvCjiQidUi0ZyT3E+na+gnQJHidFKy7L9qDmVkikSLyvLtPq6itu78PHGZm7Uut30Kk++qUYNUGM0sJ3j+FyBmLVFJyi8ZMvCCDTTv28Jvn57Jvf1HYkUSkjoi2kJwO/Mrd33P3fcHrXWA8kTOHAzIzA54Ast39nnLa9AjaYWaDgEZAvpklm1nrYH0T4GRgWbDba8BFwfJFRM6W5CD0S23FX8f259OvN3P7jKUH3kFEhCgH24mcgeSXsX4zEO3jdo8DLgAWBWMdADcDXQHc/REiNzteaGb7gN3AOHf34EzjGTOLJ1L8Jrv7jOA97gQmm9mlwGoiV5LJQRo9sDNLc7cx8f2vOLJTS8Yd1TXsSCJSy1k0k/cF08dvAy5w913BumZEnpDY0t1/WqMpq1lGRoZnZmaGHaPW2l/k/PKpT/nkq3xeHD+UwYe2DTuSiNQCZpbl7hml10fbtXUtcDSw1szeM7N3iTwxcSiRe0ykHomPMx48ZxCdWjfhiufmsn5rQdiRRKQWi3Ya+cVAT+APQCYwF/g90MPdl9RcPAlLq6aJPHZhBrv2FHL5PzMp2BfV7UIi0gBFfR+Ju+9298fc/Tp3/13wHJJUM/u0BvNJiA7v2IJ7xqWzIGcr//PKYj3DRETKVKkbEsvQFBhcHUGkdvr5kYcw4aSeTJ2bw1Mfrgw7jojUQlUtJNIATDipJz/r05E/v57Nhys2hR1HRGoZFRI5oLg4455x6RyW3IwrX5jLms27wo4kIrWIColEpXnjBCZekEFRkXPZs5ns3FMYdiQRqSUqLCRmtih4PkiZL+DlGOWUWiCtfTMePHcQX2zYzu9fXqDBdxEBDnxnuwqFfM/xhydz46m9+Mvry3jonRVcdWLPsCOJSMgqLCTufltF26VhuuxH3Vm6bht3z/qCHh2ac0rflLAjiUiINEYilWZm3Dm2PwNSW3PlC/N4ZV5O2JFEJEQqJHJQkhLjee5XRzMkrS3XvrSApz/8OuxIIhISFRI5aM0bJ/DUxUfx0z4duXX6Uu7/z3INwIs0QCokUiVJifE8fN4gxg5K5d7/fMGfZiylqEjFRKQhifZ5JCLlSoiP429n9KdlkwSe+nAl23YX8tex/UiI1+8pIg1BVIXEzC4sZ5MDBcAKd59XbamkzomLM24Z0Yc2TRtxz6wv2Fawj7+fM5CkxPiwo4lIDYv2jOQhIo+9TQSKH+YdB+wLlhPNbB5wirvnVW9EqSvMjKtP6knLpARunb6Ui5/6jMcuyqB5Y534itRn0fY9nAXMI/K43KTgdRyQBYwBBgIGlPksdmlYfnlcN+4dN4BPV27mvMc+4Zude8OOJCI1KNpCcg8wwd0/dvfC4PUx8DvgbndfAFwH/KSmgkrdMmZgKo+eP5js9ds569GP9ZRFkXos2kKSBpQ15euuYBvA10CbqkeS+uLkPh155uIh5G4tYOzDH7Fy086wI4lIDYi2kHwK3GNmhxSvCJb/D5gTrOpJ5DnuZTKzLmb2jpllm9kSM5tQRptRwYSQ880s08yGHWhfM7vVzNYG+8w3s+FRfiaJgWMOa8cLlx3Nrr2FnPHIxyxdty3sSCJSzaItJL8COgGrzWylmX0NrA7W/Spo0wy4o4L3KASuc/fewFDgSjPrU6rN28AAd08HLgEej3Lfe909PXi9HuVnkhjpn9qaKVccQ2K8cfbEj8latTnsSCJSjaIqJO6+HOgLjCQyXnIfMALo5+4rgjb/cvd/VvAeue4+N1jeDmQDnUu12eHf3RrdjMjlxVHtK7Vbjw4tmHLFMbRr3pjzHp/De1/o4j6R+iLqO8Y84i13f8Dd73f3f5f4oV8pZpZG5EqvOWVsG2Nmy4CZRM5Kotn3qqBL7Ekz0zhNLZXapimTLz+G7u2b86tnPmPGwnVhRxKRamDR1gIzOxo4CehAqQLk7ldHfUCz5sB7wJ/dfVoF7Y4HbnH3kyva18w6ApuInL3cDqS4e1kFaDwwHqBr166DV61aFW1kqWbbCvZx6dOfkbnqG/4yph/nDOkadiQRiYKZZbl7xg/WR1NIzOx64C5gBbCOoMsp4O5+YpQhEoEZwFvufsB7ToKxmKPcfVM0+wZnKzPcvW9F75uRkeGZmZnRRJYasnvvfn79fBbvfp7HDaf04tcnHBZ2JBE5gPIKSbS3HE8Arnb3B6sQwIAngOwKCkEP4Et3dzMbRORu+vyK9jWzFHfPDb4dAyw+2IwSO00axTPxggyum7KAv765jC2793LjKb2I/FWLSF0SbSFpCVT1aqjjgAuARWY2P1h3M9AVwN0fAcYCF5rZPmA3MC4oKsPK2je4QusuM0sncpa0Eri8ijklRholxHHfuHRaJiXw6HtfsW33Pu4Y3Y/4OBUTkbok2kIyCTgF+MfBHsjdPyAyjUpFbf4K/LUy+7r7BQebScIXH2fcMbovrZsm8tA7X7JtdyH3jkunUYJmDhapK6ItJGuA28zsOGAh303WCEA04x0i5TEzfv/zXrRqkshfXl/G9j2FPHL+IJo20mSPInVBtIPtFT1H1d29e/VFqnkabK+9XvpsNTdNW8TArm148qKjaNU0MexIIhKo0mC7u3er/kgiPzTuqK60SEpkwovzGDfxY569dAgdWiSFHUtEKqCOaKl1hvdL4clfHsWq/F2c9cjHrNlc1nyhIlJblNu1ZWYPADe5+85guVyVuSGxNlDXVt2QteobLn7qU5o0iue5S4+mZ8cWYUcSadDK69qq6IykH5EnIhYvl/eq8OY/kYM1+NA2TL7iGIocznz0Yxas2RJ2JBEpQ9RTpNQnOiOpW1bn7+K8Jz5h8469PHZRBsce1j7sSCIN0sGckYjUCl3bNeXlK44ltU1TfvnUZ/x7yfqwI4lICVEXEjMbZ2YTzexfZvZayVdNBhQB6NgyiZcuH0qflJb8+vm5vJxV7jPURCTGoiokZvY34Dkij9XdAuSXeonUuNZNG/H8r47mmO7tuH7KAp78oKLbm0QkVqK9dfhC4Bx3f7kmw4gcSLPGCTzxywwmTJrPn2YsZcvufVx7ck9N9igSomi7tuKA+TWYQyRqjRPiefDcgZw5OJUH3l7Ora8toaio4V00IlJbRFtIJgLn12QQkcpIiI/jrjP686th3Xjm41Vc/PRnbNqxJ+xYIg1StF1brYFzzeynlD1pY526IVHqBzPjf07rTVr7ZvxpxlJOvX82941L57geujxYJJaiPSPpQ6Rray/QC92QKLWEmXH+0EN59crjaJmUwPlPzOH/3vqcwv1FYUcTaTB0Q6LUG7v2FnLra0uYnJlDxqFtuP+cgXRu3STsWCL1hm5IlHqvaaME7jpjAPefnU527jaG3z+bt3TzokiNK3eMJLjR8Hx333agmw7d/fRqTyZykEald2ZAamuumjSXy/+ZxS+PTePGU3uRlBgfdjSReqmiM5J8Is9BL16u6CVSq6S1b8bUXx/LpcO68fRHK/nFPz7iy7wdYccSqZc0RiL13tvZG7h+ygL2FBZx+6i+jB2cGnYkkTpJYyTSYJ3UuyNvTDiefp1bcd2UBfzupfns3FMYdiyReqMykzb+JJi08U0z+2/JV5T7dzGzd8ws28yWmNmEMtqMMrOFZjbfzDLNbNiB9jWztmY2y8yWB1/bRPuZpOE4pFUSL1w2lGtO7sm/5q9lxN8/YPHarWHHEqkXop208ZfAG0AL4AQgD2gDDAKWRnmsQuA6d+8NDAWuNLM+pdq8DQxw93TgEuDxKPa9EXjb3XsG+98YZR5pYOLjjGtOPpwXLhvKrr2F/OIfH/H0h1/TELt3RapTtGck1wNXufs5RO5qv8ndBxKZETiqEUx3z3X3ucHydiAb6FyqzQ7/7n91M4LB/gPsOwp4Jlh+Bhgd5WeSBmpo93a8MeF4hvVsz63TlzL+n1ls2bU37FgidVa0haQ78J9geQ/QPFh+EPhlZQ9qZmnAQGBOGdvGmNkyYCaRs5ID7dvR3XMhUnCADuUcc3zQXZaZl5dX2chSz7Rt1ognLsrg/43ow7ufb2T4/bP5bOXmsGOJ1EnRFpJ8It1aAGv5blqUdkClbh02s+bAVOAad99Weru7v+LuvYicWdxemX0r4u4T3T3D3TOSk5Mrs6vUU2bGpcO6MfXXx5KYEMe4Rz/m728vZ79mEhaplGgLyWzgZ8HyZOABM3sKmATMivZgZpZIpBA87+7TKmrr7u8Dh5lZ+wPsu8HMUoI2KcDGaPOIAPRPbc2M3w5j5IBO3D3rC85/fA4bthWEHUukzoi2kFxFpGgA/C/wNyJnI5OBX0XzBhZ58tATQLa731NOmx5BO8xsENAIyD/Avq8BFwXLFwGvRvmZRL7VIimR+8alc9cZ/Zm/ZgvD75/NO5/rdxKRaBzwhkQzSwDGA/9y93UHfaDIpbyzgUVA8dSsNwNdAdz9ETO7gcjTGPcBu4Hfu/sH5e3r7q+bWXFB6wqsBs509wo7u3VDolRkxcbtXPXCPJat387447tz/c+OoFGCbrkSKe+GxKjubDeznUAfd19VE+FiTYVEDqRg337umLmU5z5ZzYAurfn72QPp2q5p2LFEQlXVO9s/AQZXbySR2ispMZ47Rvfj4fMG8VXeDk57YDYzFh70CblIvRbtExIfA/7PzLoCWcDOkhuL7/EQqW9O7ZdC386tuPrFeVz1wjw+XLGJW0YcSZNGmklYpFiFXVtm9iRwDbClgvdwd69T/6vUtSWVtW9/EffM+oKH3/2Snh2a8+C5gzjikBYH3lGkHjnYrq2LgCSgWwWv7tUbVaT2SYyP44ZTevHsJUP4ZtdeTn/wAyZ9ulrTq4hw4EJiAO6+qqJXDHKK1ArHH57M6xN+xJBubblp2iKumjSPbQX7wo4lEqpoBtv1K5dICR1aJPHMxUP4wylH8Obi9Zz2wGzmr9kSdiyR0ERTSNab2f6KXjWeUqSWiYszfnNCDyZffgxFRXDGwx8x8f0v1dUlDVI0V22Np+LBdpEGa/ChbXj96h9xw9SF/OX1Zcxfs4W/nTGAZo2jvSBSpO6L5l/7dHfXXBEi5WjVNJGHzx/E47O/5n/fyOarvJ08dmEGXdrqBkZpGA7UtaXzdJEomBmXHd+dpy4ewrotuzn9wQ/46MtNYccSiYmortoSkej8+PBkXr1qGO2aN+aCJz7VExilQaiwkLh7nLq1RCqnW/tmvPKbY/nJEcncOn0pN0xdyJ5CXZMi9ZemNBWpAS2SEpl4QQa/PbEHkzNzOGfiJ2zUM06knlIhEakhcXHGdT87gn+cN4js3O2c/uCHLND9JlIPqZCI1LDh/VKY+utjSYg3znz0Y6bNzQk7kki1UiERiYE+nVry2lXDGNS1Nb+bvIA/z1xK4f6iA+8oUgeokIjESNtmjfjnpUdz0TGH8tjsr7n46c/Ysmtv2LFEqkyFRCSGEuPjuG1UX+78RT8++SqfUQ99yBcbtocdS6RKVEhEQnD2kK68OH4oO/fsZ8xDH/LvJevDjiRy0FRIREIy+NC2TP/tcRzWoTnj/5nFA28vp6hINy9K3aNCIhKilFZNmHz5MYwZ2Jl7Zn3BlS/MZeeewrBjiVRKzAqJmXUxs3fMLNvMlpjZhDLajDKzhWY238wyzWxYiW1PmtlGM1tcap9bzWxtsM98Mxsei88jUl2SEuO556wB/H+n9eatJesZ+/BHrNm8K+xYIlGL5RlJIXCdu/cGhgJXmlmfUm3eBga4ezpwCfB4iW1PA6eU8973unt68Hq9emOL1Dwz41c/6s7TJSd9XKFJH6VuiFkhcfdcd58bLG8HsoHOpdrs8O9muGtGidmH3f19YHOM4oqE4vjDk3mteNLHJzXpo9QNoYyRmFkaMBCYU8a2MWa2DJhJ5KwkGlcFXWJPmlmbco45Puguy8zLyzvY6CI1Lu3bSR87aNJHqRNiXkjMrDkwFbjG3beV3u7ur7h7L2A0cHsUb/kwcBiQDuQCd5fVyN0nunuGu2ckJycfZHqR2IhM+jiYq4NJH8/WpI9Si8W0kJhZIpEi8ry7T6uobdCVdZiZtT9Auw3uvt/di4DHgCHVFlgkRHFxxu+CSR+X5W5n5IMfMF+TPkotFMurtgx4Ash293vKadMjaIeZDQIaAfkHeN+UEt+OARaX11akLhreL4VpvzmWxPg4znr0Y6ZmadJHqV1ieUZyHHABcGLJS3XN7AozuyJoMxZYbGbzgYeAccWD72Y2CfgYOMLMcszs0mCfu8xskZktBH4CXBvDzyQSE71TIpM+Du7ahuumLOD2GZr0UWoPa4hXhGRkZHhmZmbYMUQqbd/+Iv48M5unP1rJsB7tefDcgbRu2ijsWNJAmFmWu2eUXq8720XqkMT4OG49/Uj+OrYfc77O5/QHNemjhE+FRKQOGndUZNLHXXsjkz6+pUkfJUQqJCJ1VMlJHy//Zxb3/0eTPko4EsIOICIHr3jSx5umLeLe/3xBdu427j5rAM0aH9x/7aIiZ787Re4UFVFi2dlf5BQ5FHlkeX+R4x5pE1n2EsuQlBhHSqsmB51F6g79DYvUccWTPh7ZqSV/eT2bH921mRZJCZEf/N8WhhJFIigIkcLg3xaGmjqZaZmUQEqrJqS0TiKlVRM6tUrikFZJdGrdhJRWkXVNGsXXzMElJlRIROqB4kkfe6e0ZHLmGgDizYiLs+ArxJkRH2fEmQXLkXXftTHijBLLJdoU71uqTfnvbezaW8i6LQXkbt3Nui0FrN+2m0U5W8nf+cPHC7dumvhtkSkuOMVFplPrSOFpnKBiU1upkIjUI8f1aM9xPSqcDCJ0Bfv2s35rAblbI0Umd2sB67bsZv3WAtZtLSBr9Tds2bXvB/u1a9ao1FlNpMgUF52OLZNolKBh3zCokIhITCUlxpPWvhlp7ZuV22bX3kJytxZEisuW3d8rOqvzd/HJV/lsL/j+A8DMoH3zxpGzmm+70pLISGvLwC6tCSbNkBqgQiIitU7TRgkcltycw5Kbl9tmx55CcksUmeJutNytBazI28Hs5Xns3BuZNblHh+aclZHKmIGpJLdoHKuP0WDoznYRqZfcnS279vHvpeuZnJlD1qpviI8zTuzVgbMyunDCEckkxqsrrDLKu7NdhUREGoQVG3cwJWsNU7PWsmnHHto3b8zYQZ05M6MLPTqUf+Yj31EhKUGFRKTh2re/iPc+z2Ny5hr+u2wjhUXOoK6tGXdUF07r34nmuu+lXCokJaiQiAhA3vY9vDIvh8mZOazYuIMmifGc1j+FszK6cFRaGw3Ql6JCUoIKiYiU5O7MW7OFKZlrmL4glx17Cklr15QzM7owdlAqh7RKCjtiraBCUoIKiYiUZ9feQt5YtJ7JmWuY8/Vm4gx+fHgyZ2V04aTeHRv0vSoqJCWokIhINFZu2snLWTm8nJXD+m0FtG3WiDEDO3NmRiq9DmkZdryYUyEpQYVERCpjf5Eze3keUzJz+PfS9ezb7/RPbcVZGV0YOaATrZokhh0xJlRISlAhEZGDtXnnXv41by2TM9ewbP12GifEcWrfQzgrowtDu7cjLq7+DtCrkJSgQiIiVeXuLF67jcmZa/jX/LVsLygktU0TzhzchTMyUuncuknYEaudCkkJKiQiUp0K9u3nrSXrmZKZwwcrNmEGw3q058yMLvysT0eSEuvHzMUqJCWokIhITVmzede3A/Rrt+ymVZNERqd3YvTAzqTX8ckjQy8kZtYFeBY4BCgCJrr7/aXajAJuD7YXAte4+wfBtieBEcBGd+9bYp+2wEtAGrASOMvdv6koiwqJiNS0oiLnoy/zmZy5hjeXrGdvYRGpbZowckAnRvbvRO+UFnWuqNSGQpICpLj7XDNrAWQBo919aYk2zYGd7u5m1h+Y7O69gm3HAzuAZ0sVkruAze5+p5ndCLRx9xsqyqJCIiKxtHX3PmYt3cD0Bev4YMUm9hc5hyU3Y0T/Towc0KnOzPUVeiH5wYHNXgUedPdZ5Ww/BnjS3XuXWJcGzChVSD4HTnD33KBYvevuR1R0bBUSEQnL5p17eWNxLtMXrGPO15txh94pLRk5IIWR/TvRpW3TsCOWq1YVkqAgvA/0dfdtpbaNAf4X6ACc5u4fl9qvdCHZ4u6tS3z/jbu3KeOY44HxAF27dh28atWq6vxIIiKVtmFbATMX5jJ94Trmrd4CQHqX1owc0InT+qXUuqlZak0hCbqv3gP+7O7TKmh3PHCLu59cYl0aB1lIStIZiYjUNms272LmosiZypJ12zCDo9LaMnJAJ4b3PYR2zcN/IFetKCRmlgjMAN5y93uiaP81cJS7bwq+T0NdWyJSz32Zt4MZC3J5bcFavszbSXyccexh7Rg5oBM/P/KQ0O6kD72QWOTyhGeIDIxfU06bHsCXwWD7IGA6kOpByHIKyd+A/BKD7W3d/Q8VZVEhEZG6wN1Ztn47MxauY/qCXFZv3kVivPHjw5MZOaATJ/fuSLMYPj+lNhSSYcBsYBGRy3sBbga6Arj7I2Z2A3AhsA/YDfy+xOW/k4ATgPbABuCP7v6EmbUDJgfvsxo40903V5RFhURE6hp3Z2HOVqYvWMeMhbms31ZAUmIcJ/XqyMgBKZxwRIcav/Ex9EJSm6iQiEhdVlTkZK76hukL1vH6olzyd+6leeMEftonUlSG9UiukenuVUhKUCERkfqicH8RH3+Vz4wFubyxOJdtBYW0apLIqX0PYeSATgzt3o74appIUoWkBBUSEamP9hYWMXt5HtMXrGPW0g3s3Luf9s0bc1q/QxgxoBODu7ap0uzEKiQlqJCISH23e+9+3vl8I9MXrOO/yzayp7CIlFZJ3H3mAI7t0f6g3rO8QhK74X4REYmZJo3iGd4vheH9Utixp5D/LN3AawvW1cid8yokIiL1XPPGCYwe2JnRAzvXyPs33KfYi4hItVAhERGRKlEhERGRKlEhERGRKlEhERGRKlEhERGRKlEhERGRKlEhERGRKmmQU6SYWR5wsM/abQ9sqsY4B0s5vq825KgNGUA5SlOO76tKjkPdPbn0ygZZSKrCzDLLmmtGOZSjNmRQDuUII4e6tkREpEpUSEREpEpUSCpvYtgBAsrxfbUhR23IAMpRmnJ8X7Xn0BiJiIhUic5IRESkSlRIRESkSlRIomRmT5rZRjNbHHKOLmb2jpllm9kSM5sQQoYkM/vUzBYEGW6LdYZSeeLNbJ6ZzQgxw0ozW2Rm880stOc4m1lrM3vZzJYF/0aOCSHDEcGfQ/Frm5ldE0KOa4N/n4vNbJKZJcU6Q5BjQpBhSSz/HMr6mWVmbc1slpktD762qY5jqZBE72nglLBDAIXAde7eGxgKXGlmfWKcYQ9worsPANKBU8xsaIwzlDQByA7x+MV+4u7pId8rcD/wprv3AgYQwp+Lu38e/DmkA4OBXcArscxgZp2Bq4EMd+8LxANnxzJDkKMvcBkwhMjfxwgz6xmjwz/ND39m3Qi87e49gbeD76tMhSRK7v4+sLkW5Mh197nB8nYiPyhq5vmZ5Wdwd98RfJsYvEK5asPMUoHTgMfDOH5tYmYtgeOBJwDcfa+7bwk1FJwEfOnuBzuTRFUkAE3MLAFoCqwLIUNv4BN33+XuhcB7wJhYHLicn1mjgGeC5WeA0dVxLBWSOszM0oCBwJwQjh1vZvOBjcAsd495hsB9wB+AopCOX8yBf5tZlpmNDylDdyAPeCro6nvczJqFlKXY2cCkWB/U3dcC/wesBnKBre7+71jnABYDx5tZOzNrCgwHuoSQo1hHd8+FyC+lQIfqeFMVkjrKzJoDU4Fr3H1brI/v7vuDrotUYEhwCh9TZjYC2OjuWbE+dhmOc/dBwKlEuhuPDyFDAjAIeNjdBwI7qaaui4NhZo2A04EpIRy7DZHfvrsBnYBmZnZ+rHO4ezbwV2AW8CawgEj3dL2iQlIHmVkikSLyvLtPCzNL0HXyLuGMHx0HnG5mK4EXgRPN7LkQcuDu64KvG4mMBwwJIUYOkFPi7PBlIoUlLKcCc919QwjHPhn42t3z3H0fMA04NoQcuPsT7j7I3Y8n0tW0PIwcgQ1mlgIQfN1YHW+qQlLHmJkR6QPPdvd7QsqQbGatg+UmRP7TLot1Dne/yd1T3T2NSBfKf9095r91mlkzM2tRvAz8jEiXRky5+3pgjZkdEaw6CVga6xwlnEMI3VqB1cBQM2sa/J85iZAuyDCzDsHXrsAvCO/PBOA14KJg+SLg1ep404TqeJOGwMwmAScA7c0sB/ijuz8RQpTjgAuARcEYBcDN7v56DDOkAM+YWTyRX0Ymu3tol97WAh2BVyI/r0gAXnD3N0PK8lvg+aBb6Svg4jBCBOMBPwUuD+P47j7HzF4G5hLpSppHeFOUTDWzdsA+4Ep3/yYWBy3rZxZwJzDZzC4lUmzPrJZjaYoUERGpCnVtiYhIlaiQiIhIlaiQiIhIlaiQiIhIlaiQiIhIlaiQiNRxZuZmdkbYOaThUiERqQIzezr4QV769UnY2URiRTckilTdf4jcJFrS3jCCiIRBZyQiVbfH3deXem2Gb7udrjKzmWa2y8xWlZ480Mz6mdl/zGy3mW0OznJalWpzUfDgrD1mtsHMni6Voa2ZTTGznWb2VRgTFErDpUIiUvNuIzLHUTqRaTqeNbMM+HYqkTeBHUQmehxDZHLBJ4t3NrPLgUeBp4D+RKYiX1LqGLcQmTdpAPAS8KSZHVpjn0ikBE2RIlIFwZnB+UBBqU0PufsNZubA4+5+WYl9/gOsd/fzzewyIs/NSA0eVIaZnQC8A/R09xXBPEnPuXuZU8IHx7jT3W8Kvk8AtgHj3T2U2ZClYdEYiUjVvQ+UfpjVlhLLH5fa9jGRpzpC5Al6C4uLSOAjIg/q6mNm24g8AfPtA2RYWLzg7oVmlkc1PbRI5EBUSESqbpe7rzjIfY3yH1PswfZo7CtjX3VdS0zoH5pIzRtaxvfFz8ZYCgwofp5J4Fgi/zezg4dCrSXyPA2RWklnJCJV19jMDim1br+75wXLvzCzz4g8SfIMIkXh6GDb80QG4581s1uANkQG1qeVOMv5M3CvmW0AZgJNgZPc/e6a+kAilaFCIlJ1JwO5pdatJfI8e4BbgbHAA0AecLG7fwbg7rvM7OfAfcCnRAbtXwUmFL+Ruz9sZnuB64g8/3szEMsHmYlUSFdtidSg4IqqM9395bCziNQUjZGIiEiVqJCIiEiVqGtLRESqRGckIiJSJSokIiJSJSokIiJSJSokIiJSJSokIiJSJf8/n3KL5hUmofwAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_losses(\"MAP\", model.all_losses(), plt.subplot())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('bdl2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dcfaf3907fb5ccaba69729c1b2eca8858ecf7d628290008c66c1f9d42d557f9b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
