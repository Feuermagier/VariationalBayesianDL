{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from training.util import sgd, plot_losses, adam, lr_scheduler, wilson_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import experiments.base.cifar as cifar\n",
    "\n",
    "batch_size = 128\n",
    "trainloader = cifar.cifar10_trainloader(\"/mnt/d/Uni/Bachelorarbeit/linux/data/\", batch_size=batch_size, subsample=5000)\n",
    "testloader = cifar.cifar10_testloader(\"/mnt/d/Uni/Bachelorarbeit/linux/data/\", batch_size=batch_size)\n",
    "stl_testloader = cifar.stl10_testloader(\"/mnt/d/Uni/Bachelorarbeit/linux/data/\", batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import experiments.base.cifar as cifar\n",
    "corrupted_testloader = cifar.cifar10_corrupted_testloader(\"/mnt/d/Uni/Bachelorarbeit/linux/data/\", intensity=4, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 0: loss 2.0147554874420166\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 1: loss 1.880865454673767\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 2: loss 1.7365572452545166\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 3: loss 1.7241096496582031\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 4: loss 1.586887001991272\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 5: loss 1.5148136615753174\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 6: loss 1.4849541187286377\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 7: loss 1.432898759841919\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 8: loss 1.2941534519195557\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 9: loss 1.237328290939331\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 10: loss 1.227186918258667\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 11: loss 1.2092366218566895\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 12: loss 1.1710546016693115\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 13: loss 1.1930068731307983\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 14: loss 1.0800745487213135\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 15: loss 1.0307790040969849\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 16: loss 1.094902753829956\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 17: loss 0.9638921022415161\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 18: loss 0.9590792655944824\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 19: loss 0.947908878326416\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 20: loss 0.9403896331787109\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 21: loss 0.8866109848022461\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 22: loss 0.9416518211364746\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 23: loss 0.8631836771965027\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 24: loss 0.8826999664306641\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 25: loss 0.8139163851737976\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 26: loss 0.787857711315155\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 27: loss 0.78573077917099\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 28: loss 0.7651749849319458\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 29: loss 0.7416888475418091\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 30: loss 0.7605226635932922\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 31: loss 0.7087138891220093\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 32: loss 0.6634750962257385\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 33: loss 0.6527236104011536\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 34: loss 0.656269907951355\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 35: loss 0.5831039547920227\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 36: loss 0.6566165089607239\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 37: loss 0.5710057616233826\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 38: loss 0.5996950268745422\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 39: loss 0.5839815735816956\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 40: loss 0.5604400038719177\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 41: loss 0.6428526639938354\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 42: loss 0.6034493446350098\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 43: loss 0.506456196308136\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 44: loss 0.4915699064731598\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 45: loss 0.5465339422225952\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 46: loss 0.6381558179855347\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 47: loss 0.5069711804389954\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 48: loss 0.5995613932609558\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 49: loss 0.5261092782020569\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 50: loss 0.5942054390907288\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 51: loss 0.48550862073898315\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 52: loss 0.5123732686042786\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 53: loss 0.4797784388065338\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 54: loss 0.41269636154174805\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 55: loss 0.4318297505378723\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 56: loss 0.3711227774620056\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 57: loss 0.32496821880340576\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 58: loss 0.3998415172100067\n",
      "Adjusting learning rate of group 0 to 1.0000e-01.\n",
      "Epoch 59: loss 0.4368850588798523\n",
      "Adjusting learning rate of group 0 to 9.7938e-02.\n",
      "Epoch 60: loss 0.36825093626976013\n",
      "Adjusting learning rate of group 0 to 9.5875e-02.\n",
      "Epoch 61: loss 0.3946099877357483\n",
      "Adjusting learning rate of group 0 to 9.3813e-02.\n",
      "Epoch 62: loss 0.463795006275177\n",
      "Adjusting learning rate of group 0 to 9.1750e-02.\n",
      "Epoch 63: loss 0.49153614044189453\n",
      "Adjusting learning rate of group 0 to 8.9688e-02.\n",
      "Epoch 64: loss 0.3515166640281677\n",
      "Adjusting learning rate of group 0 to 8.7625e-02.\n",
      "Epoch 65: loss 0.31411901116371155\n",
      "Adjusting learning rate of group 0 to 8.5562e-02.\n",
      "Epoch 66: loss 0.34544113278388977\n",
      "Adjusting learning rate of group 0 to 8.3500e-02.\n",
      "Epoch 67: loss 0.4484935402870178\n",
      "Adjusting learning rate of group 0 to 8.1438e-02.\n",
      "Epoch 68: loss 0.37829700112342834\n",
      "Adjusting learning rate of group 0 to 7.9375e-02.\n",
      "Epoch 69: loss 0.3149575889110565\n",
      "Adjusting learning rate of group 0 to 7.7313e-02.\n",
      "Epoch 70: loss 0.31763601303100586\n",
      "Adjusting learning rate of group 0 to 7.5250e-02.\n",
      "Epoch 71: loss 0.3280028700828552\n",
      "Adjusting learning rate of group 0 to 7.3188e-02.\n",
      "Epoch 72: loss 0.29465538263320923\n",
      "Adjusting learning rate of group 0 to 7.1125e-02.\n",
      "Epoch 73: loss 0.4111699163913727\n",
      "Adjusting learning rate of group 0 to 6.9063e-02.\n",
      "Epoch 74: loss 0.2896692454814911\n",
      "Adjusting learning rate of group 0 to 6.7000e-02.\n",
      "Epoch 75: loss 0.2897338569164276\n",
      "Adjusting learning rate of group 0 to 6.4937e-02.\n",
      "Epoch 76: loss 0.1854139268398285\n",
      "Adjusting learning rate of group 0 to 6.2875e-02.\n",
      "Epoch 77: loss 0.22706103324890137\n",
      "Adjusting learning rate of group 0 to 6.0813e-02.\n",
      "Epoch 78: loss 0.3662296235561371\n",
      "Adjusting learning rate of group 0 to 5.8750e-02.\n",
      "Epoch 79: loss 0.33498021960258484\n",
      "Adjusting learning rate of group 0 to 5.6687e-02.\n",
      "Epoch 80: loss 0.2821499705314636\n",
      "Adjusting learning rate of group 0 to 5.4625e-02.\n",
      "Epoch 81: loss 0.27735787630081177\n",
      "Adjusting learning rate of group 0 to 5.2563e-02.\n",
      "Epoch 82: loss 0.23077157139778137\n",
      "Adjusting learning rate of group 0 to 5.0500e-02.\n",
      "Epoch 83: loss 0.23011374473571777\n",
      "Adjusting learning rate of group 0 to 4.8437e-02.\n",
      "Epoch 84: loss 0.16383185982704163\n",
      "Adjusting learning rate of group 0 to 4.6375e-02.\n",
      "Epoch 85: loss 0.1509142816066742\n",
      "Adjusting learning rate of group 0 to 4.4313e-02.\n",
      "Epoch 86: loss 0.14989981055259705\n",
      "Adjusting learning rate of group 0 to 4.2250e-02.\n",
      "Epoch 87: loss 0.19227032363414764\n",
      "Adjusting learning rate of group 0 to 4.0188e-02.\n",
      "Epoch 88: loss 0.22427137196063995\n",
      "Adjusting learning rate of group 0 to 3.8125e-02.\n",
      "Epoch 89: loss 0.13619089126586914\n",
      "Adjusting learning rate of group 0 to 3.6063e-02.\n",
      "Epoch 90: loss 0.10394986718893051\n",
      "Adjusting learning rate of group 0 to 3.4000e-02.\n",
      "Epoch 91: loss 0.09163758903741837\n",
      "Adjusting learning rate of group 0 to 3.1938e-02.\n",
      "Epoch 92: loss 0.10461322963237762\n",
      "Adjusting learning rate of group 0 to 2.9875e-02.\n",
      "Epoch 93: loss 0.08393978327512741\n",
      "Adjusting learning rate of group 0 to 2.7813e-02.\n",
      "Epoch 94: loss 0.11410476267337799\n",
      "Adjusting learning rate of group 0 to 2.5750e-02.\n",
      "Epoch 95: loss 0.08291912823915482\n",
      "Adjusting learning rate of group 0 to 2.3687e-02.\n",
      "Epoch 96: loss 0.0686584934592247\n",
      "Adjusting learning rate of group 0 to 2.1625e-02.\n",
      "Epoch 97: loss 0.08883543312549591\n",
      "Adjusting learning rate of group 0 to 1.9563e-02.\n",
      "Epoch 98: loss 0.07774626463651657\n",
      "Adjusting learning rate of group 0 to 1.7500e-02.\n",
      "Epoch 99: loss 0.0915553942322731\n",
      "Adjusting learning rate of group 0 to 1.5438e-02.\n",
      "Epoch 100: loss 0.10173974186182022\n",
      "Adjusting learning rate of group 0 to 1.3375e-02.\n",
      "Epoch 101: loss 0.08289817720651627\n",
      "Adjusting learning rate of group 0 to 1.1313e-02.\n",
      "Epoch 102: loss 0.052136801183223724\n",
      "Adjusting learning rate of group 0 to 9.2500e-03.\n",
      "Epoch 103: loss 0.05546671152114868\n",
      "Adjusting learning rate of group 0 to 7.1875e-03.\n",
      "Epoch 104: loss 0.060942839831113815\n",
      "Adjusting learning rate of group 0 to 5.1250e-03.\n",
      "Epoch 105: loss 0.04785251244902611\n",
      "Adjusting learning rate of group 0 to 3.0625e-03.\n",
      "Epoch 106: loss 0.040510185062885284\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 107: loss 0.04191116616129875\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 108: loss 0.034782927483320236\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 109: loss 0.044435013085603714\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 110: loss 0.03913208842277527\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 111: loss 0.04134664684534073\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 112: loss 0.03799585625529289\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 113: loss 0.04035433381795883\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 114: loss 0.05405557155609131\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 115: loss 0.03743932396173477\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 116: loss 0.04006580263376236\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 117: loss 0.04063451290130615\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 118: loss 0.04287613555788994\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 119: loss 0.052864063531160355\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 120: loss 0.04219452291727066\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 121: loss 0.05589237064123154\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 122: loss 0.0912884920835495\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 123: loss 0.0440712533891201\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 124: loss 0.0403011180460453\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 125: loss 0.03461291640996933\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 126: loss 0.04777713865041733\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 127: loss 0.031541600823402405\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 128: loss 0.04097897931933403\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 129: loss 0.042208313941955566\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 130: loss 0.045190904289484024\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 131: loss 0.0365201011300087\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 132: loss 0.04072609916329384\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 133: loss 0.04663712903857231\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 134: loss 0.042834024876356125\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 135: loss 0.03416062146425247\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 136: loss 0.042073801159858704\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 137: loss 0.040079645812511444\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 138: loss 0.03216699883341789\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 139: loss 0.04507315158843994\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 140: loss 0.040142424404621124\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 141: loss 0.04109559580683708\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 142: loss 0.03312771022319794\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 143: loss 0.04110565409064293\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 144: loss 0.03287972882390022\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 145: loss 0.029864436015486717\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 146: loss 0.03345511108636856\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 147: loss 0.040230680257081985\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 148: loss 0.04806732386350632\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 149: loss 0.031651582568883896\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 150: loss 0.040923137217760086\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 151: loss 0.03530540689826012\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 152: loss 0.0668405294418335\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 153: loss 0.039077065885066986\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 154: loss 0.03322364762425423\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 155: loss 0.06886468827724457\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 156: loss 0.03408496826887131\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 157: loss 0.051746465265750885\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 158: loss 0.035574205219745636\n",
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 159: loss 0.03321406990289688\n",
      "Final loss 0.03321406990289688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MAP(\n",
       "  (model): Sequential(\n",
       "    (0): PreResNet(\n",
       "      (model): Sequential(\n",
       "        (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): PreBasicBlock(\n",
       "          (main_path): Sequential(\n",
       "            (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (1): ReLU()\n",
       "            (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (4): ReLU()\n",
       "            (5): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (skip_path): Identity()\n",
       "        )\n",
       "        (2): PreBasicBlock(\n",
       "          (main_path): Sequential(\n",
       "            (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (1): ReLU()\n",
       "            (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (4): ReLU()\n",
       "            (5): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (skip_path): Identity()\n",
       "        )\n",
       "        (3): PreBasicBlock(\n",
       "          (main_path): Sequential(\n",
       "            (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (1): ReLU()\n",
       "            (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (4): ReLU()\n",
       "            (5): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (skip_path): Identity()\n",
       "        )\n",
       "        (4): PreBasicBlock(\n",
       "          (main_path): Sequential(\n",
       "            (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (1): ReLU()\n",
       "            (2): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (4): ReLU()\n",
       "            (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (skip_path): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        )\n",
       "        (5): PreBasicBlock(\n",
       "          (main_path): Sequential(\n",
       "            (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (1): ReLU()\n",
       "            (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (4): ReLU()\n",
       "            (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (skip_path): Identity()\n",
       "        )\n",
       "        (6): PreBasicBlock(\n",
       "          (main_path): Sequential(\n",
       "            (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (1): ReLU()\n",
       "            (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (4): ReLU()\n",
       "            (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (skip_path): Identity()\n",
       "        )\n",
       "        (7): PreBasicBlock(\n",
       "          (main_path): Sequential(\n",
       "            (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (1): ReLU()\n",
       "            (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (4): ReLU()\n",
       "            (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (skip_path): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        )\n",
       "        (8): PreBasicBlock(\n",
       "          (main_path): Sequential(\n",
       "            (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (1): ReLU()\n",
       "            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (4): ReLU()\n",
       "            (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (skip_path): Identity()\n",
       "        )\n",
       "        (9): PreBasicBlock(\n",
       "          (main_path): Sequential(\n",
       "            (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (1): ReLU()\n",
       "            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "            (4): ReLU()\n",
       "            (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "          (skip_path): Identity()\n",
       "        )\n",
       "        (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        (11): ReLU()\n",
       "        (12): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
       "        (13): Flatten(start_dim=1, end_dim=-1)\n",
       "        (14): Linear(in_features=64, out_features=10, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (1): LogSoftmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from training.pp import MAP\n",
    "\n",
    "layers = [\n",
    "    (\"preresnet-20\", (32, 3, 10)),\n",
    "    (\"logsoftmax\", ())\n",
    "]\n",
    "\n",
    "model = MAP(layers)\n",
    "\n",
    "model.train_model(160, torch.nn.NLLLoss(), sgd(1e-1, weight_decay=1e-4, momentum=0.9, nesterov=True), trainloader, batch_size, device, scheduler_factory=wilson_scheduler(160, 0.1, None))\n",
    "#model.load_state_dict(torch.load(\"/mnt/d/Uni/Bachelorarbeit/results/CIFAR10/2/results/MAP/log/rep_00model.tar\"))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.43424999713897705\n",
      " Avg Log Likelihood: -5.012221813201904\n",
      " Avg Likelihood: 0.4180564284324646\n",
      " ECE: 0.4057955401577055\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.4342),\n",
       " tensor(-5.0122),\n",
       " tensor(0.4181),\n",
       " <training.calibration.ClassificationCalibrationResults at 0x7f6c63d8fb20>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import experiments.base.multiclass_classification as exp\n",
    "\n",
    "exp.eval_model(model, 5, stl_testloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.bbb import BBBModel, GaussianPrior\n",
    "\n",
    "prior = GaussianPrior(torch.tensor(0), torch.tensor(1))\n",
    "layers = [\n",
    "    (\"variational-preresnet-20\", (32, 3, 10, prior)),\n",
    "    (\"logsoftmax\", ())\n",
    "]\n",
    "\n",
    "model = BBBModel(layers)\n",
    "\n",
    "model.train_model(120, torch.nn.NLLLoss(), sgd(1e-1), trainloader, batch_size, device, scheduler_factory=lr_scheduler([80, 120], 0.1), mc_samples=2, kl_rescaling=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5264)\n",
      " Accuracy: 0.37957367300987244\n",
      " Avg Log Likelihood: -2.0528364181518555\n",
      " Avg Likelihood: 0.30053094029426575\n",
      " ECE: 0.146800649462405\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.3796),\n",
       " tensor(-2.0528),\n",
       " tensor(0.3005),\n",
       " <training.calibration.ClassificationCalibrationResults at 0x7fcc00161f70>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import experiments.base.multiclass_classification as exp\n",
    "\n",
    "model.to(device)\n",
    "exp.eval_model(model, 5, corrupted_testloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.720300018787384\n",
      " Avg Log Likelihood: -1.1909916400909424\n",
      " Avg Likelihood: 0.6958616971969604\n",
      " ECE: 0.15854743110537528\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.7203),\n",
       " tensor(-1.1910),\n",
       " tensor(0.6959),\n",
       " <training.calibration.ClassificationCalibrationResults at 0x7f6c75a63c40>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import experiments.base.multiclass_classification as exp\n",
    "\n",
    "exp.eval_model(model, 5, testloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss 2.3485770225524902\n",
      "Epoch 1: loss 2.34991717338562\n",
      "Epoch 2: loss 2.3480513095855713\n",
      "Epoch 3: loss 2.3499252796173096\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/flo/VariationalBayesianDL/CIFAR.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/flo/VariationalBayesianDL/CIFAR.ipynb#ch0000005vscode-remote?line=3'>4</a>\u001b[0m layers \u001b[39m=\u001b[39m [\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/flo/VariationalBayesianDL/CIFAR.ipynb#ch0000005vscode-remote?line=4'>5</a>\u001b[0m     (\u001b[39m\"\u001b[39m\u001b[39mpreresnet-20\u001b[39m\u001b[39m\"\u001b[39m, (\u001b[39m32\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m10\u001b[39m)),\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/flo/VariationalBayesianDL/CIFAR.ipynb#ch0000005vscode-remote?line=5'>6</a>\u001b[0m     (\u001b[39m\"\u001b[39m\u001b[39mlogsoftmax\u001b[39m\u001b[39m\"\u001b[39m, ())\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/flo/VariationalBayesianDL/CIFAR.ipynb#ch0000005vscode-remote?line=6'>7</a>\u001b[0m ]\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/flo/VariationalBayesianDL/CIFAR.ipynb#ch0000005vscode-remote?line=8'>9</a>\u001b[0m model \u001b[39m=\u001b[39m VOGNModule(layers)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/flo/VariationalBayesianDL/CIFAR.ipynb#ch0000005vscode-remote?line=9'>10</a>\u001b[0m model\u001b[39m.\u001b[39;49mtrain_model(\u001b[39m10\u001b[39;49m, torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mNLLLoss(), {\u001b[39m\"\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m1e-6\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mprior_prec\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m150\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mbetas\u001b[39;49m\u001b[39m\"\u001b[39;49m: (\u001b[39m0.9\u001b[39;49m, \u001b[39m0.999\u001b[39;49m), \u001b[39m\"\u001b[39;49m\u001b[39mdamping\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m1\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39maugmentation\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m1\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39msample\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mFalse\u001b[39;49;00m}, trainloader, batch_size, device, mc_samples\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n",
      "File \u001b[0;32m~/VariationalBayesianDL/training/vogn.py:115\u001b[0m, in \u001b[0;36mVOGNModule.train_model\u001b[0;34m(self, epochs, loss_fn, optim_params, loader, batch_size, device, report_every_epochs, mc_samples)\u001b[0m\n\u001b[1;32m    112\u001b[0m data, target \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device), target\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    113\u001b[0m data, target \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mexpand(mc_samples, \u001b[39m*\u001b[39mdata\u001b[39m.\u001b[39mshape), target\u001b[39m.\u001b[39mexpand(mc_samples, \u001b[39m*\u001b[39mtarget\u001b[39m.\u001b[39mshape)\n\u001b[0;32m--> 115\u001b[0m grads, loss \u001b[39m=\u001b[39m vmap(run_sample, (\u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, \u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m), randomness\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdifferent\u001b[39;49m\u001b[39m\"\u001b[39;49m)(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptim_state, data, target)\n\u001b[1;32m    116\u001b[0m loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()\n\u001b[1;32m    117\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_state \u001b[39m=\u001b[39m vogn_step(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams, grads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_state)\n",
      "File \u001b[0;32m~/anaconda3/envs/bdl2/lib/python3.9/site-packages/functorch/_src/vmap.py:383\u001b[0m, in \u001b[0;36mvmap.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    382\u001b[0m     batched_inputs \u001b[39m=\u001b[39m _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)\n\u001b[0;32m--> 383\u001b[0m     batched_outputs \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49mbatched_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    384\u001b[0m     \u001b[39mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n\u001b[1;32m    385\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/VariationalBayesianDL/training/vogn.py:107\u001b[0m, in \u001b[0;36mVOGNModule.train_model.<locals>.run_sample\u001b[0;34m(parameters, optim_state, input, target)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_sample\u001b[39m(parameters, optim_state, \u001b[39minput\u001b[39m, target):\n\u001b[1;32m    106\u001b[0m     prep_params \u001b[39m=\u001b[39m vogn_prepare(parameters, optim_state)\n\u001b[0;32m--> 107\u001b[0m     \u001b[39mreturn\u001b[39;00m vmap(grad_and_value(get_loss), (\u001b[39mNone\u001b[39;49;00m, \u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m))(prep_params, \u001b[39minput\u001b[39;49m, target)\n",
      "File \u001b[0;32m~/anaconda3/envs/bdl2/lib/python3.9/site-packages/functorch/_src/vmap.py:383\u001b[0m, in \u001b[0;36mvmap.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    382\u001b[0m     batched_inputs \u001b[39m=\u001b[39m _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)\n\u001b[0;32m--> 383\u001b[0m     batched_outputs \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49mbatched_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    384\u001b[0m     \u001b[39mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n\u001b[1;32m    385\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/bdl2/lib/python3.9/site-packages/functorch/_src/eager_transforms.py:1074\u001b[0m, in \u001b[0;36mgrad_and_value.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1072\u001b[0m \u001b[39m# NB: need create_graph so that backward pass isn't run in no_grad mode\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m flat_outputs \u001b[39m=\u001b[39m _as_tuple(output)\n\u001b[0;32m-> 1074\u001b[0m flat_grad_input \u001b[39m=\u001b[39m _autograd_grad(flat_outputs, flat_diff_args, create_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   1075\u001b[0m grad_input \u001b[39m=\u001b[39m tree_unflatten(flat_grad_input, spec)\n\u001b[1;32m   1077\u001b[0m grad_input \u001b[39m=\u001b[39m _undo_create_differentiable(grad_input, level)\n",
      "File \u001b[0;32m~/anaconda3/envs/bdl2/lib/python3.9/site-packages/functorch/_src/eager_transforms.py:93\u001b[0m, in \u001b[0;36m_autograd_grad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(diff_outputs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     92\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(torch\u001b[39m.\u001b[39mzeros_like(inp) \u001b[39mfor\u001b[39;00m inp \u001b[39min\u001b[39;00m inputs)\n\u001b[0;32m---> 93\u001b[0m grad_inputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mgrad(diff_outputs, inputs, grad_outputs,\n\u001b[1;32m     94\u001b[0m                                   retain_graph\u001b[39m=\u001b[39;49mretain_graph,\n\u001b[1;32m     95\u001b[0m                                   create_graph\u001b[39m=\u001b[39;49mcreate_graph,\n\u001b[1;32m     96\u001b[0m                                   allow_unused\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     97\u001b[0m grad_inputs \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(torch\u001b[39m.\u001b[39mzeros_like(inp) \u001b[39mif\u001b[39;00m gi \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m gi\n\u001b[1;32m     98\u001b[0m                     \u001b[39mfor\u001b[39;00m gi, inp \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(grad_inputs, inputs))\n\u001b[1;32m     99\u001b[0m \u001b[39mreturn\u001b[39;00m grad_inputs\n",
      "File \u001b[0;32m~/anaconda3/envs/bdl2/lib/python3.9/site-packages/torch/autograd/__init__.py:275\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[39mreturn\u001b[39;00m _vmap_internals\u001b[39m.\u001b[39m_vmap(vjp, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, allow_none_pass_through\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)(grad_outputs)\n\u001b[1;32m    274\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 275\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    276\u001b[0m         outputs, grad_outputs_, retain_graph, create_graph, inputs,\n\u001b[1;32m    277\u001b[0m         allow_unused, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from training.pp import MAP\n",
    "from training.vogn import iVONModuleFunctorch, VOGNModule\n",
    "\n",
    "layers = [\n",
    "    (\"preresnet-20\", (32, 3, 10)),\n",
    "    (\"logsoftmax\", ())\n",
    "]\n",
    "\n",
    "model = VOGNModule(layers)\n",
    "model.train_model(10, torch.nn.NLLLoss(), {\"lr\": 1e-4, \"prior_prec\": 150, \"betas\": (0.9, 0.999), \"damping\": 1, \"augmentation\": 1, \"sample\": False}, trainloader, batch_size, device, mc_samples=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7874)\n",
      " Accuracy: 0.5201526284217834\n",
      " Avg Log Likelihood: -2.049781560897827\n",
      " Avg Likelihood: 0.493396133184433\n",
      " ECE: 0.26724077704540994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.5202),\n",
       " tensor(-2.0498),\n",
       " tensor(0.4934),\n",
       " <training.calibration.ClassificationCalibrationResults at 0x7fd9bd87dfa0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import experiments.base.multiclass_classification as exp\n",
    "\n",
    "model.to(device)\n",
    "exp.eval_model(model, 5, corrupted_testloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_losses(\"MAP\", model.all_losses(), plt.subplot())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('bdl2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dcfaf3907fb5ccaba69729c1b2eca8858ecf7d628290008c66c1f9d42d557f9b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
